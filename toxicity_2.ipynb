{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toxicity Dataset : https://archive.ics.uci.edu/dataset/728/toxicity-2\n",
    "\n",
    "The dataset includes 171 molecules designed for functional domains of a core clock protein, CRY1, responsible for generating circadian rhythm. 56 of the molecules are toxic and the rest are non-toxic. \n",
    "\n",
    "The data consists a complete set of 1203 molecular descriptors and needs feature selection before classification since some of the features are redundant. \n",
    "\n",
    "Introductory Paper:\n",
    "Structure-based design and classifications of small molecules regulating the circadian rhythm period\n",
    "By Seref Gul, F. Rahim, Safak Isin, Fatma Yilmaz, Nuri Ozturk, M. Turkay, I. Kavakli. 2021\n",
    "https://www.semanticscholar.org/paper/Structure-based-design-and-classifications-of-small-Gul-Rahim/5944836c47bc7d1a2b0464a9a1db94d4bc7f28ce\n",
    "Published in Scientific reports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:07.970883Z",
     "iopub.status.busy": "2025-09-18T04:07:07.970487Z",
     "iopub.status.idle": "2025-09-18T04:07:07.979245Z",
     "shell.execute_reply": "2025-09-18T04:07:07.977849Z",
     "shell.execute_reply.started": "2025-09-18T04:07:07.970858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report, roc_curve\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the toxicity dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:12.702310Z",
     "iopub.status.busy": "2025-09-18T04:07:12.701902Z",
     "iopub.status.idle": "2025-09-18T04:07:14.072099Z",
     "shell.execute_reply": "2025-09-18T04:07:14.070911Z",
     "shell.execute_reply.started": "2025-09-18T04:07:12.702270Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (171, 1204)\n",
      "\n",
      "First few rows:\n",
      "   MATS3v  nHBint10  MATS3s  MATS3p  nHBDon_Lipinski  minHBint8  MATS3e  \\\n",
      "0  0.0908         0  0.0075  0.0173                0        0.0 -0.0436   \n",
      "1  0.0213         0  0.1144 -0.0410                0        0.0  0.1231   \n",
      "2  0.0018         0 -0.0156 -0.0765                2        0.0 -0.1138   \n",
      "3 -0.0251         0 -0.0064 -0.0894                3        0.0 -0.0747   \n",
      "4  0.0135         0  0.0424 -0.0353                0        0.0 -0.0638   \n",
      "\n",
      "   MATS3c  minHBint2  MATS3m  ...   WTPT-4   WTPT-5  ETA_EtaP_L  ETA_EtaP_F  \\\n",
      "0  0.0409        0.0  0.1368  ...   0.0000   0.0000      0.1780      1.5488   \n",
      "1 -0.0316        0.0  0.1318  ...   8.8660  19.3525      0.1739      1.3718   \n",
      "2 -0.1791        0.0  0.0615  ...   5.2267  27.8796      0.1688      1.4395   \n",
      "3 -0.1151        0.0  0.0361  ...   7.7896  24.7336      0.1702      1.4654   \n",
      "4  0.0307        0.0  0.0306  ...  12.3240  19.7486      0.1789      1.4495   \n",
      "\n",
      "   ETA_EtaP_B  nT5Ring  SHdNH  ETA_dEpsilon_C  MDEO-22     Class  \n",
      "0      0.0088        0    0.0         -0.0868     0.00  NonToxic  \n",
      "1      0.0048        2    0.0         -0.0810     0.25  NonToxic  \n",
      "2      0.0116        2    0.0         -0.1004     0.00  NonToxic  \n",
      "3      0.0133        2    0.0         -0.1010     0.00  NonToxic  \n",
      "4      0.0120        2    0.0         -0.1071     0.00  NonToxic  \n",
      "\n",
      "[5 rows x 1204 columns]\n",
      "\n",
      "Column names:\n",
      "['MATS3v', 'nHBint10', 'MATS3s', 'MATS3p', 'nHBDon_Lipinski', 'minHBint8', 'MATS3e', 'MATS3c', 'minHBint2', 'MATS3m', 'minHBint6', 'minHBint7', 'minHBint4', 'MATS3i', 'VR3_Dt', 'SpMax8_Bhi', 'SdsN', 'SpMax8_Bhm', 'SpMax8_Bhe', 'ECCEN', 'MDEC-14', 'SpMax8_Bhs', 'SpMax8_Bhp', 'SpMax8_Bhv', 'MDEC-11', 'MDEC-12', 'MDEC-13', 'VR2_Dt', 'BIC5', 'ATS7s', 'ATS7p', 'ATS7v', 'ATS7i', 'ATS7m', 'ATS7e', 'mintN', 'nHsNH2', 'khs.sssCH', 'minHBint3', 'maxdssC', 'nT6Ring', 'minHBint5', 'nF8Ring', 'minssCH2', 'SpMax_DzZ', 'ETA_EtaP', 'nHsOH', 'SpMin1_Bhe', 'maxHother', 'nHBAcc_Lipinski', 'StN', 'khs.aaS', 'khs.aaO', 'khs.aaN', 'Sare', 'SHAvin', 'SpMax3_Bhv', 'SpMax3_Bhp', 'SpMax3_Bhs', 'SpMax3_Bhe', 'SpMin6_Bhi', 'SpMax3_Bhm', 'SpMax3_Bhi', 'ETA_EtaP_F_L', 'mindCH2', 'AATSC2e', 'AATSC2c', 'AATSC2m', 'AATSC2i', 'nsBr', 'AATS5p', 'AATSC2v', 'AATSC2p', 'AATSC2s', 'VABC', 'maxdNH', 'khs.ddsN', 'RotBtFrac', 'ATS4e', 'ATS4m', 'nFRing', 'ATS4i', 'EE_DzZ', 'ATS4s', 'ATS4p', 'ETA_Alpha', 'khs.sssN', 'EE_Dzi', 'MAXDN', 'EE_Dzm', 'EE_Dze', 'EE_Dzs', 'EE_Dzp', 'EE_Dzv', 'ATS8e', 'maxsOH', 'minssssNp', 'maxsOm', 'MDEC-23', 'MDEC-22', 'MDEC-24', 'nFG12HeteroRing', 'ATS8s', 'ATS8v', 'SP-6', 'SP-7', 'SHsNH2', 'SP-5', 'SP-2', 'SP-3', 'SP-0', 'SP-1', 'minHsOH', 'ATSC8v', 'MATS2v', 'ATSC8s', 'MATS2p', 'MATS2s', 'ATSC8p', 'MATS2e', 'ATSC8e', 'ATSC8c', 'MATS2c', 'MATS2m', 'topoDiameter', 'ATSC8m', 'MATS2i', 'ATSC8i', 'ntN', 'khs.ssCH2', 'SpAD_Dt', 'ETA_Eta_R_L', 'SHdsCH', 'SaasN', 'SC-4', 'SaasC', 'minaaCH', 'AATSC3c', 'AATSC3e', 'AATSC3i', 'AATSC3m', 'AATSC3s', 'AATSC3p', 'AATSC3v', 'SpMax2_Bhp', 'nF8HeteroRing', 'AATS8e', 'AATS8i', 'AATS8m', 'AATS8s', 'AATS8p', 'AATS8v', 'VE3_Dt', 'XLogP', 'SpMax2_Bhi', 'maxssCH2', 'minaaS', 'SpMax4_Bhv', 'SpMax4_Bhs', 'SpMax4_Bhp', 'SpMax4_Bhm', 'minHaaNH', 'SpMax4_Bhi', 'minaaN', 'SpMax4_Bhe', 'StsC', 'SssCH2', 'maxHdNH', 'MATS1p', 'R_TpiPCTPC', 'MATS1s', 'MATS1v', 'JGI10', 'MATS1c', 'MATS1e', 'VR2_DzZ', 'MATS1i', 'MATS1m', 'MDEC-34', 'MDEC-33', 'VR2_Dze', 'VR2_Dzm', 'VR2_Dzs', 'VR2_Dzp', 'khs.ssssC', 'nTG12Ring', 'khs.ssssN', 'ATS5e', 'gmin', 'VR2_D', 'ATS5m', 'ATS5p', 'ATS5s', 'ATS5v', 'AATSC5p', 'TpiPC', 'maxsCH3', 'SdS', 'khs.ssO', 'ETA_Eta_F_L', 'khs.ssS', 'SdO', 'VE2_Dt', 'maxHtCH', 'SpMax_Dze', 'SpMax_Dzm', 'SpMax_Dzi', 'ETA_dEpsilon_B', 'SpMax_Dzv', 'ETA_dEpsilon_A', 'SpMax_Dzs', 'ETA_dEpsilon_D', 'SpMax_Dzp', 'SsNH2', 'StCH', 'SsCH3', 'CIC5', 'CIC4', 'CIC1', 'CIC0', 'CIC3', 'CIC2', 'nF10HeteroRing', 'maxssO', 'WPOL', 'n5HeteroRing', 'maxHAvin', 'fragC', 'ETA_Eta_B_RC', 'AATS7m', 'SpDiam_Dt', 'SdssC', 'ETA_Epsilon_3', 'AATS7i', 'AATS7e', 'nT9Ring', 'minsCl', 'AATS7v', 'AATS7s', 'AATS7p', 'nHdCH2', 'ETA_Epsilon_5', 'ETA_Epsilon_4', 'SsssCH', 'maxHsOH', 'GATS1v', 'maxaaaC', 'GATS1s', 'minsNH2', 'BIC4', 'SpMin7_Bhs', 'SpMin7_Bhp', 'SpMin7_Bhv', 'nHtCH', 'GATS1e', 'mintsC', 'GATS1c', 'SpMin7_Bhm', 'GATS1m', 'SpMin7_Bhe', 'GATS1i', 'maxtsC', 'minHAvin', 'MDEC-44', 'AATS2v', 'SPC-6', 'SPC-4', 'SPC-5', 'SpAD_D', 'MATS6c', 'ETA_BetaP_s', 'minaasC', 'minaasN', 'minssNH', 'nT7HeteroRing', 'RotBFrac', 'nF10Ring', 'ETA_BetaP_ns', 'nH', 'nL', 'nN', 'nO', 'nA', 'nC', 'nF', 'nX', 'nQ', 'nS', 'nV', 'ATS1m', 'SdNH', 'mindsN', 'SHCsats', 'SHCsatu', 'CrippenMR', 'GATS1p', 'SRW10', 'ETA_dPsi_A', 'AATS6m', 'AATS6i', 'AATS6e', 'minsBr', 'nF9HeteroRing', 'SpMin7_Bhi', 'AATS6v', 'AATS6p', 'AATS6s', 'naAromAtom', 'nBase', 'minHBint10', 'SpDiam_DzZ', 'SaaNH', 'nssssC', 'khs.dNH', 'maxaaN', 'maxaaO', 'SpDiam_Dzv', 'SpDiam_Dzs', 'SpDiam_Dzp', 'SpDiam_Dze', 'SpDiam_Dzm', 'GATS6i', 'SpDiam_Dzi', 'Mi', 'Mv', 'Mp', 'GGI10', 'nBr', 'bpol', 'MW', 'GATS6v', 'MATS7s', 'MATS7p', 'C1SP1', 'MATS7v', 'MATS7i', 'MATS7m', 'MATS7c', 'GATS6s', 'MATS7e', 'maxtN', 'SpMin8_Bhe', 'SpMin8_Bhi', 'SpMin8_Bhm', 'SpMin8_Bhp', 'SpMin8_Bhs', 'SpMin8_Bhv', 'maxHCsatu', 'maxHCsats', 'ATSC3v', 'ATSC3s', 'ATSC3p', 'minHdCH2', 'ATSC3e', 'ATSC3c', 'maxssNH', 'ATSC3m', 'ATSC3i', 'minHCsatu', 'minHCsats', 'SpMax7_Bhe', 'SpMax7_Bhi', 'SpMax7_Bhm', 'ETA_BetaP_ns_d', 'SpMax7_Bhp', 'SpMax7_Bhs', 'SpMax7_Bhv', 'SdsCH', 'minssO', 'minssS', 'SpMin3_Bhe', 'SpMin3_Bhm', 'SpMin3_Bhi', 'SpMin3_Bhv', 'nT8Ring', 'SpMin3_Bhs', 'SpMin3_Bhp', 'TPC', 'khs.tCH', 'VP-5', 'VP-4', 'VP-7', 'VP-6', 'VP-1', 'VP-0', 'VP-3', 'VP-2', 'MIC5', 'MIC4', 'MIC3', 'MIC2', 'MIC1', 'MIC0', 'ATSC5p', 'piPC10', 'ATSC5s', 'minsOm', 'nT10HeteroRing', 'nHBa', 'nHBd', 'SddssS', 'nCl', 'minsOH', 'SHaaCH', 'nHBDon', 'nF11HeteroRing', 'AATS5i', 'AATS5m', 'SpMin6_Bhs', 'SpMin6_Bhp', 'SpMin6_Bhv', 'AATS5e', 'ETA_dBeta', 'khs.sCH3', 'ALogP', 'SpMin6_Bhm', 'BCUTp-1l', 'AATS5s', 'BCUTp-1h', 'AATS5v', 'SpMin6_Bhe', 'ATS8i', 'ATS8m', 'BCUTw-1h', 'BCUTw-1l', 'nBondsS3', 'nBondsS2', 'ATS8p', 'GATS3p', 'GATS3s', 'GATS3v', 'GATS3c', 'GATS3e', 'SC-5', 'GATS3i', 'SC-6', 'GATS3m', 'SC-3', 'minsCH3', 'SssssC', 'nAtomLC', 'nT12HeteroRing', 'minHaaCH', 'MLFER_BH', 'MLFER_BO', 'SaaaC', 'mindsCH', 'nddssS', 'maxaasC', 'maxsssN', 'MATS6p', 'MATS6s', 'MATS6v', 'MATS6i', 'MATS6m', 'MATS6e', 'ETA_Beta_ns_d', 'hmax', 'ETA_Beta_s', 'nHaaCH', 'khs.aaaC', 'khs.sNH2', 'ETA_AlphaP', 'nAromBond', 'ATSC2v', 'ATSC2p', 'ATSC2s', 'ATS4v', 'ATSC2e', 'ATSC2c', 'ATSC2m', 'ATSC2i', 'AATS4v', 'AATS4s', 'AATS4p', 'AATS4e', 'ETA_BetaP', 'AATS4m', 'khs.sOH', 'AATS4i', 'SHsOH', 'SpMax_D', 'MDEN-13', 'MDEN-12', 'MDEN-11', 'ntsC', 'ATS3m', 'PetitjeanNumber', 'khs.aasN', 'khs.aasC', 'MWC10', 'MPC7', 'TWC', 'topoRadius', 'WPATH', 'nG', 'ndsN', 'MAXDP', 'naaaC', 'SM1_DzZ', 'SpAbs_DzZ', 'SpAbs_Dze', 'khs.aaNH', 'SpAbs_Dzm', 'SM1_Dzv', 'SpAbs_Dzi', 'SM1_Dzp', 'SM1_Dzs', 'SM1_Dzm', 'SpAbs_Dzv', 'SM1_Dzi', 'SpAbs_Dzp', 'SpAbs_Dzs', 'SM1_Dze', 'maxaaCH', 'maxdO', 'nF11Ring', 'GATS2v', 'GATS2s', 'ETA_Beta', 'GATS2m', 'GATS2i', 'maxdS', 'GATS2e', 'GATS2c', 'nHaaNH', 'ATSC1p', 'ATSC1s', 'ATSC1v', 'ATSC1c', 'ATSC1e', 'ATSC1i', 'ATSC1m', 'MWC9', 'MWC8', 'MWC5', 'MWC4', 'MWC7', 'MWC6', 'ndssC', 'MWC2', 'MAXDN2', 'MATS5e', 'SpMin2_Bhe', 'MATS5c', 'MATS5m', 'SpMin2_Bhm', 'MATS5i', 'SpMin2_Bhi', 'SpMin2_Bhv', 'MATS5v', 'MATS5p', 'SpMin2_Bhs', 'SpMin2_Bhp', 'VE1_Dzi', 'Mare', 'BCUTc-1l', 'BCUTc-1h', 'SssNH', 'MPC9', 'VE1_Dzp', 'naaN', 'naaO', 'naaS', 'SsBr', 'SHBd', 'TSRW', 'SHBa', 'khs.ssNH', 'nT9HeteroRing', 'SpMax6_Bhp', 'SpMax6_Bhs', 'SpMAD_Dzv', 'SpMax6_Bhv', 'SpMax6_Bhe', 'SpMAD_Dze', 'SpMax6_Bhi', 'SpMAD_Dzi', 'SpMax6_Bhm', 'SpMAD_Dzm', 'SpMAD_DzZ', 'AATSC8s', 'AATSC8p', 'AATSC8v', 'minHssNH', 'AATSC8c', 'AATSC8e', 'AATSC8i', 'AATSC8m', 'AATS3s', 'AATS3p', 'AATS3v', 'SpMin5_Bhv', 'SpMin5_Bhp', 'SpMin5_Bhs', 'SpMin5_Bhm', 'SpMin5_Bhi', 'AATS3e', 'SpMin5_Bhe', 'AATS3i', 'SpMAD_Dzs', 'AATS3m', 'n7Ring', 'maxdCH2', 'MLFER_L', 'GATS5s', 'GATS5p', 'GATS5v', 'MLFER_A', 'GATS5i', 'GATS5m', 'IC3', 'IC2', 'IC1', 'IC0', 'khs.tN', 'IC5', 'IC4', 'ETA_Eta_R', 'ETA_Eta_L', 'SHdCH2', 'ETA_Eta_B', 'ETA_Eta_F', 'SaaCH', 'nAcid', 'maxsssCH', 'SsCl', 'nF6Ring', 'maxsF', 'MATS4c', 'MATS4e', 'MATS4i', 'SP-4', 'MATS4m', 'MATS4s', 'MATS4p', 'MATS4v', 'minHdNH', 'VC-6', 'SsssN', 'minaaaC', 'minwHBa', 'nT6HeteroRing', 'khs.dssC', 'AATSC0m', 'EE_Dt', 'minsF', 'ATSC0v', 'ATSC0s', 'ATSC0p', 'ATSC0m', 'ATSC0i', 'ATSC0e', 'ATSC0c', 'VE2_D', 'SpMAD_D', 'nHBint2', 'nHBint3', 'ETA_Epsilon_1', 'nHBint6', 'nHBint7', 'nHBint4', 'nHBint5', 'nHBint8', 'nHBint9', 'nBondsS', 'AATS2p', 'AATS2s', 'nBondsT', 'AATS2e', 'nBondsD', 'AATS2i', 'AATS2m', 'nBondsM', 'nBonds2', 'MDEN-33', 'Sp', 'Sv', 'Si', 'FMF', 'nRotB', 'n7HeteroRing', 'minssssC', 'SwHBa', 'Zagreb', 'SRW6', 'SRW7', 'SRW4', 'SRW5', 'SRW2', 'khs.sCl', 'SRW8', 'SRW9', 'maxHsNH2', 'SCH-5', 'SCH-6', 'SCH-7', 'khs.sF', 'nF9Ring', 'ATS6p', 'maxaasN', 'MAXDP2', 'minHtCH', 'nsNH2', 'nFG12Ring', 'VE1_DzZ', 'VE1_Dze', 'GATS4p', 'GATS4s', 'VE1_Dzm', 'GATS4v', 'GATS4i', 'VE1_Dzs', 'GATS4m', 'ndsCH', 'VE1_Dzv', 'GATS4c', 'GATS4e', 'ETA_Beta_ns', 'McGowan_Volume', 'n5Ring', 'ETA_dAlpha_B', 'ATSC7s', 'ATSC7p', 'ETA_dAlpha_A', 'ATSC7v', 'ATSC7i', 'ATSC7m', 'ATSC7c', 'VC-4', 'VC-5', 'VC-3', 'ATSC7e', 'maxssssNp', 'JGI2', 'JGI3', 'JGI1', 'JGI6', 'JGI7', 'JGI4', 'JGI5', 'JGI8', 'JGI9', 'SpAD_DzZ', 'SpAD_Dze', 'SpAD_Dzm', 'SpAD_Dzi', 'SpAD_Dzv', 'SpAD_Dzp', 'SpAD_Dzs', 'nsssCH', 'nBonds', 'LipinskiFailures', 'SpDiam_D', 'minHdsCH', 'SssO', 'SssS', 'C2SP3', 'C2SP2', 'C2SP1', 'GATS2p', 'ETA_Shape_X', 'ETA_Shape_Y', 'ETA_Shape_P', 'naaNH', 'khs.dCH2', 'AVP-5', 'AVP-4', 'AVP-7', 'AVP-1', 'AVP-0', 'AVP-2', 'AATS1v', 'AATS1p', 'AATS1s', 'AATS1m', 'MDEN-22', 'AATS1i', 'AATS1e', 'maxaaNH', 'maxHaaNH', 'nT8HeteroRing', 'GATS7c', 'GATS7m', 'GATS7v', 'GATS7p', 'GATS7s', 'SsOm', 'SsOH', 'AATSC4v', 'maxHBd', 'AATSC4s', 'khs.sBr', 'maxHBa', 'khs.dsN', 'SHssNH', 'AATSC4e', 'AATSC4c', 'AATSC4m', 'AATSC4i', 'SHBint10', 'ATS2v', 'ATS2p', 'ATS2s', 'CrippenLogP', 'ATS2i', 'JGT', 'MPC10', 'ndNH', 'VAdjMat', 'khs.ddssS', 'nT7Ring', 'khs.dsCH', 'ATSC6p', 'ATSC6s', 'ATSC6v', 'ATSC6i', 'ATSC6m', 'ATSC6c', 'ATSC6e', 'SpMin1_Bhi', 'SpMin1_Bhm', 'SpMin1_Bhp', 'SpMin1_Bhs', 'nT11HeteroRing', 'SpMin1_Bhv', 'SHother', 'AATS0s', 'SpMin4_Bhv', 'AATS0p', 'SpMin4_Bhp', 'AATS0v', 'SpMin4_Bhs', 'SpMin4_Bhm', 'AATS0i', 'SpMin4_Bhi', 'AATS0m', 'minsssCH', 'SpMin4_Bhe', 'nAtomLAC', 'AATS0e', 'nsCH3', 'MWC3', 'ATSC5v', 'MLFER_S', 'ETA_dBetaP', 'AATSC4p', 'nRotBt', 'nsOm', 'SHaaNH', 'nsOH', 'AATSC5v', 'AATSC5s', 'nssssNp', 'AATSC5e', 'naasN', 'AATSC5c', 'AATSC5m', 'nsssN', 'SpMax_Dt', 'AATSC5i', 'naasC', 'VR3_D', 'VR1_D', 'VR1_Dt', 'ETA_Epsilon_2', 'MLogP', 'nHBAcc2', 'nHBAcc3', 'AMR', 'AMW', 'GATS6c', 'sumI', 'GATS6e', 'gmax', 'maxHssNH', 'MATS5s', 'maxsNH2', 'GATS6m', 'maxHBint8', 'maxHBint9', 'GATS6p', 'C1SP2', 'C1SP3', 'khs.tsC', 'maxHBint2', 'maxHBint3', 'maxHBint4', 'maxHBint5', 'maxHBint6', 'maxHBint7', 'AVP-6', 'nHdNH', 'ATSC5e', 'C4SP3', 'AVP-3', 'ATSC5c', 'ATSC5m', 'nHother', 'ATSC5i', 'VCH-5', 'VCH-7', 'VCH-6', 'Mse', 'nsF', 'DELS2', 'MATS8m', 'ATS3v', 'ATS3s', 'ATS3p', 'ATS3e', 'MATS8i', 'Kier2', 'Kier3', 'Kier1', 'HybRatio', 'ATS3i', 'MDEN-23', 'SHBint9', 'SHBint8', 'SHBint3', 'SHBint2', 'SHBint7', 'SHBint6', 'SHBint5', 'SHBint4', 'DELS', 'minHsNH2', 'nHeteroRing', 'GATS7e', 'ETA_Psi_1', 'mindssC', 'nwHBa', 'maxHBint10', 'mindNH', 'GATS7i', 'nHssNH', 'nAtom', 'Spe', 'khs.dS', 'khs.dO', 'maxwHBa', 'AATSC6p', 'AATSC6s', 'AATSC6v', 'AATSC6c', 'AATSC6e', 'AATSC6i', 'AATSC6m', 'SssssNp', 'AATSC7c', 'nssCH2', 'ASP-2', 'ASP-3', 'ASP-0', 'ASP-1', 'ASP-6', 'ASP-7', 'ASP-4', 'ASP-5', 'ATS0s', 'MATS8v', 'ATS0p', 'MATS8s', 'ATS0v', 'MATS8p', 'MATS8e', 'MATS8c', 'ATS0e', 'ATS0i', 'ATS0m', 'GGI9', 'GGI8', 'GGI1', 'GGI3', 'GGI2', 'GGI5', 'GGI4', 'GGI7', 'GGI6', 'nTRing', 'nT10Ring', 'TIC0', 'TIC1', 'TIC2', 'TIC3', 'TIC4', 'TIC5', 'maxsCl', 'ntCH', 'khs.aaCH', 'ZMIC4', 'ZMIC5', 'ZMIC0', 'ZMIC1', 'ZMIC2', 'ZMIC3', 'minHother', 'ATSC4c', 'ATSC4e', 'ATSC4i', 'ATSC4m', 'ATSC4s', 'ATSC4p', 'ATSC4v', 'SpMax1_Bhs', 'SpMax1_Bhp', 'SpMax1_Bhv', 'hmin', 'SpMax1_Bhe', 'SpMax1_Bhi', 'SpMax1_Bhm', 'MPC6', 'nAtomP', 'MPC4', 'MPC5', 'MPC2', 'VR3_Dzv', 'VR3_Dzs', 'MPC8', 'VR3_Dzp', 'VR3_Dzm', 'SpMAD_Dzp', 'VR3_Dzi', 'VR3_Dze', 'SsF', 'minaaO', 'VR3_DzZ', 'mintCH', 'Sse', 'C3SP2', 'VR1_DzZ', 'VR1_Dzp', 'VR1_Dzs', 'SdCH2', 'SHtCH', 'VR1_Dzi', 'VR1_Dzm', 'VR1_Dze', 'maxdsN', 'AATSC7v', 'AATSC7s', 'AATSC7p', 'AATSC7m', 'AATSC7i', 'nHdsCH', 'AATSC7e', 'BIC2', 'BIC3', 'BIC0', 'BIC1', 'SIC5', 'SIC4', 'SIC1', 'SIC0', 'SIC3', 'SIC2', 'naaCH', 'minHBd', 'minHBa', 'AATSC1p', 'nssO', 'AATSC0i', 'ALogp2', 'nssS', 'AATSC1i', 'SaaS', 'SaaO', 'SaaN', 'maxsBr', 'GATS8m', 'SpMax2_Bhv', 'GATS8i', 'SpMax2_Bhs', 'GATS8e', 'GATS8c', 'SpMax2_Bhe', 'SpMax2_Bhm', 'GATS8v', 'GATS8p', 'GATS8s', 'piPC3', 'piPC2', 'piPC1', 'piPC7', 'piPC6', 'piPC5', 'piPC4', 'piPC9', 'piPC8', 'maxHdsCH', 'nT11Ring', 'ATS1v', 'ATS1p', 'ATS1s', 'ATS1i', 'SpMAD_Dt', 'ATS1e', 'VR2_Dzi', 'n6HeteroRing', 'MPC3', 'ATS2e', 'VR2_Dzv', 'minHBint9', 'VE2_Dzs', 'VE2_Dzp', 'VE2_Dzv', 'ATS2m', 'VE2_Dzi', 'VE2_Dzm', 'maxtCH', 'minaaNH', 'VE2_Dze', 'VE2_DzZ', 'nHAvin', 'meanI', 'nHCsatu', 'nHCsats', 'apol', 'ATS5i', 'nF12Ring', 'mindS', 'mindO', 'MolIP', 'maxdsCH', 'ndCH2', 'TopoPSA', 'ETA_EtaP_B_RC', 'MDEO-11', 'MDEO-12', 'AATSC0s', 'SpMax5_Bhv', 'AATSC0p', 'SpMax5_Bhs', 'AATSC0v', 'SpMax5_Bhp', 'SpMax5_Bhm', 'SpMax5_Bhi', 'AATSC0c', 'SpMax5_Bhe', 'AATSC0e', 'EE_D', 'VE3_DzZ', 'C3SP3', 'VE3_Dzv', 'minddssS', 'ATS6s', 'VE3_Dzs', 'VE3_Dzp', 'ATS6v', 'ATS6i', 'VE3_Dzm', 'ATS6m', 'VE3_Dzi', 'VE3_Dze', 'ATS6e', 'nTG12HeteroRing', 'VPC-6', 'VPC-5', 'VPC-4', 'MLFER_E', 'nBondsD2', 'nT12Ring', 'Mpe', 'maxHdCH2', 'nT5HeteroRing', 'nHeavyAtom', 'nRing', 'GATS5c', 'nF12HeteroRing', 'GATS5e', 'VR1_Dzv', 'minsssN', 'topoShape', 'ndO', 'ndS', 'nssNH', 'VE1_Dt', 'maxHaaCH', 'nHBAcc', 'VE3_D', 'nsCl', 'VE1_D', 'AATSC1s', 'AATSC1v', 'AATSC1m', 'AATSC1c', 'AATSC1e', 'LipoaffinityIndex', 'n6Ring', 'ETA_Eta', 'WTPT-1', 'WTPT-2', 'WTPT-3', 'WTPT-4', 'WTPT-5', 'ETA_EtaP_L', 'ETA_EtaP_F', 'ETA_EtaP_B', 'nT5Ring', 'SHdNH', 'ETA_dEpsilon_C', 'MDEO-22', 'Class']\n",
      "\n",
      "Features shape: (171, 1203)\n",
      "Target shape: (171,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(\"./data.csv\")\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(data.head())\n",
    "print(\"\\nColumn names:\")\n",
    "print(data.columns.tolist())\n",
    "\n",
    "# Separate features and target\n",
    "# Assuming the last column or a column named 'Class' contains the target\n",
    "if 'Class' in data.columns:\n",
    "    X = data.drop('Class', axis=1)\n",
    "    y = data['Class']\n",
    "else:\n",
    "    # Assume last column is the target\n",
    "    X = data.iloc[:, :-1]\n",
    "    y = data.iloc[:, -1]\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.075327Z",
     "iopub.status.busy": "2025-09-18T04:07:14.074981Z",
     "iopub.status.idle": "2025-09-18T04:07:14.091774Z",
     "shell.execute_reply": "2025-09-18T04:07:14.090085Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.075303Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DATA EXPLORATION ===\n",
      "Shape of features (X): (171, 1203)\n",
      "Shape of target (y): (171,)\n",
      "\n",
      "Target distribution:\n",
      "Class\n",
      "NonToxic    115\n",
      "Toxic        56\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class balance:\n",
      "Class\n",
      "NonToxic    0.672515\n",
      "Toxic       0.327485\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Target data type: object\n",
      "Unique target values: ['NonToxic' 'Toxic']\n"
     ]
    }
   ],
   "source": [
    "# Basic data exploration\n",
    "print(\"\\n=== DATA EXPLORATION ===\")\n",
    "print(f\"Shape of features (X): {X.shape}\")\n",
    "print(f\"Shape of target (y): {y.shape}\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(y.value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(y.value_counts(normalize=True))\n",
    "\n",
    "# Check target data type and unique values\n",
    "print(f\"\\nTarget data type: {y.dtype}\")\n",
    "print(f\"Unique target values: {y.unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.092964Z",
     "iopub.status.busy": "2025-09-18T04:07:14.092654Z",
     "iopub.status.idle": "2025-09-18T04:07:14.127069Z",
     "shell.execute_reply": "2025-09-18T04:07:14.125982Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.092939Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values in features: 0\n",
      "Missing values in target: 0\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(f\"\\nMissing values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {y.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.128382Z",
     "iopub.status.busy": "2025-09-18T04:07:14.128095Z",
     "iopub.status.idle": "2025-09-18T04:07:14.163758Z",
     "shell.execute_reply": "2025-09-18T04:07:14.162588Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.128359Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PREPROCESSING ===\n",
      "Missing values in features: 0\n",
      "Missing values in target: 0\n",
      "\n",
      "Binary target distribution:\n",
      "Class\n",
      "1    115\n",
      "0     56\n",
      "Name: count, dtype: int64\n",
      "Class balance: Class\n",
      "1    0.672515\n",
      "0    0.327485\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Mapping verification:\n",
      "Original 'NonToxic' â†’ Binary 1: [1]\n",
      "Original 'Toxic' â†’ Binary 0: [0]\n"
     ]
    }
   ],
   "source": [
    "# Handle missing values if any\n",
    "print(\"=== PREPROCESSING ===\")\n",
    "print(f\"Missing values in features: {X.isnull().sum().sum()}\")\n",
    "print(f\"Missing values in target: {y.isnull().sum()}\")\n",
    "\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    # Option 1: Drop columns with too many missing values\n",
    "    missing_threshold = 0.3  # Drop columns with >30% missing\n",
    "    missing_prop = X.isnull().sum() / len(X)\n",
    "    cols_to_drop = missing_prop[missing_prop > missing_threshold].index\n",
    "    X = X.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Option 2: Impute remaining missing values\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "    print(f\"Missing values imputed\")\n",
    "\n",
    "# Convert target to binary (1 for NonToxic, 0 for Toxic) - FLIPPED LABELS\n",
    "y_binary = (y == 'NonToxic').astype(int)\n",
    "\n",
    "# Verify the binary conversion\n",
    "print(\"\\nBinary target distribution:\")\n",
    "print(y_binary.value_counts())\n",
    "print(f\"Class balance: {y_binary.value_counts(normalize=True)}\")\n",
    "\n",
    "# Double-check the conversion is correct\n",
    "print(f\"\\nMapping verification:\")\n",
    "print(f\"Original 'NonToxic' â†’ Binary 1: {y_binary[y == 'NonToxic'].unique()}\")\n",
    "print(f\"Original 'Toxic' â†’ Binary 0: {y_binary[y == 'Toxic'].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.165321Z",
     "iopub.status.busy": "2025-09-18T04:07:14.165004Z",
     "iopub.status.idle": "2025-09-18T04:07:14.910757Z",
     "shell.execute_reply": "2025-09-18T04:07:14.909482Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.165298Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURE PREPROCESSING ===\n",
      "Removed 0 constant features\n",
      "Remaining features after variance filtering: 994\n",
      "Removed 434 highly correlated features\n",
      "Final feature count: 560\n"
     ]
    }
   ],
   "source": [
    "# Feature preprocessing\n",
    "print(\"\\n=== FEATURE PREPROCESSING ===\")\n",
    "\n",
    "# Remove constant features\n",
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "X_filtered = constant_filter.fit_transform(X)\n",
    "constant_columns = X.columns[~constant_filter.get_support()]\n",
    "print(f\"Removed {len(constant_columns)} constant features\")\n",
    "\n",
    "# Remove quasi-constant features (variance < 0.01)\n",
    "quasi_constant_filter = VarianceThreshold(threshold=0.01)\n",
    "X_filtered = quasi_constant_filter.fit_transform(X_filtered)\n",
    "selected_features = X.columns[constant_filter.get_support()][quasi_constant_filter.get_support()]\n",
    "X_filtered = pd.DataFrame(X_filtered, columns=selected_features)\n",
    "print(f\"Remaining features after variance filtering: {X_filtered.shape[1]}\")\n",
    "\n",
    "# Remove highly correlated features\n",
    "correlation_matrix = X_filtered.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "high_corr_features = [column for column in upper_triangle.columns \n",
    "                      if any(upper_triangle[column] > 0.95)]\n",
    "X_filtered = X_filtered.drop(columns=high_corr_features)\n",
    "print(f\"Removed {len(high_corr_features)} highly correlated features\")\n",
    "print(f\"Final feature count: {X_filtered.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.912087Z",
     "iopub.status.busy": "2025-09-18T04:07:14.911762Z",
     "iopub.status.idle": "2025-09-18T04:07:14.946771Z",
     "shell.execute_reply": "2025-09-18T04:07:14.945787Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.912060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (136, 560)\n",
      "Test set size: (35, 560)\n",
      "\n",
      "Train set class distribution:\n",
      "Class\n",
      "1    0.669118\n",
      "0    0.330882\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set class distribution:\n",
      "Class\n",
      "1    0.685714\n",
      "0    0.314286\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Split the data with stratification to ensure balanced folds\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Add some randomness to address potential ordering issues\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(len(X_filtered))\n",
    "X_shuffled = X_filtered.iloc[shuffle_idx].reset_index(drop=True)\n",
    "y_shuffled = y_binary.iloc[shuffle_idx].reset_index(drop=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_shuffled, y_shuffled, test_size=0.2, random_state=42, \n",
    "    stratify=y_shuffled, shuffle=True\n",
    ")\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_scaled.shape}\")\n",
    "print(f\"Test set size: {X_test_scaled.shape}\")\n",
    "\n",
    "# Check class distribution in train and test sets\n",
    "print(f\"\\nTrain set class distribution:\")\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.948302Z",
     "iopub.status.busy": "2025-09-18T04:07:14.947976Z",
     "iopub.status.idle": "2025-09-18T04:07:14.956524Z",
     "shell.execute_reply": "2025-09-18T04:07:14.955228Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.948279Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"Evaluate a classification model and return metrics\"\"\"\n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Probabilities for AUC\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_train_proba = model.predict_proba(X_train)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_train_proba = model.decision_function(X_train)\n",
    "        y_test_proba = model.decision_function(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'model': model_name,\n",
    "        'train_accuracy': accuracy_score(y_train, y_train_pred),\n",
    "        'test_accuracy': accuracy_score(y_test, y_test_pred),\n",
    "        'train_auc': roc_auc_score(y_train, y_train_proba),\n",
    "        'test_auc': roc_auc_score(y_test, y_test_proba),\n",
    "        'precision': precision_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred),\n",
    "        'f1': f1_score(y_test, y_test_pred)\n",
    "    }\n",
    "    \n",
    "    return metrics, y_test_pred, y_test_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.960480Z",
     "iopub.status.busy": "2025-09-18T04:07:14.960129Z",
     "iopub.status.idle": "2025-09-18T04:07:14.990137Z",
     "shell.execute_reply": "2025-09-18T04:07:14.988942Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.960456Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "results = []\n",
    "all_predictions = {}\n",
    "all_probabilities = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T04:07:14.991219Z",
     "iopub.status.busy": "2025-09-18T04:07:14.990992Z",
     "iopub.status.idle": "2025-09-18T04:07:15.021271Z",
     "shell.execute_reply": "2025-09-18T04:07:15.019782Z",
     "shell.execute_reply.started": "2025-09-18T04:07:14.991202Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON: ORDINARY VS PENALIZED REGRESSION\n",
      "================================================================================\n",
      "\n",
      "This analysis compares the following models:\n",
      "1. Ordinary Logistic Regression (no regularization) - Baseline\n",
      "2. Ridge Regression (L2 penalty) - Shrinks coefficients\n",
      "3. Lasso Regression (L1 penalty) - Feature selection + shrinkage  \n",
      "4. Elastic Net (L1 + L2 penalty) - Combines both approaches\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON: ORDINARY VS PENALIZED REGRESSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "This analysis compares the following models:\n",
    "1. Ordinary Logistic Regression (no regularization) - Baseline\n",
    "2. Ridge Regression (L2 penalty) - Shrinks coefficients\n",
    "3. Lasso Regression (L1 penalty) - Feature selection + shrinkage  \n",
    "4. Elastic Net (L1 + L2 penalty) - Combines both approaches\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinary Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0. ORDINARY LOGISTIC REGRESSION (BASELINE)\n",
      "--------------------------------------------------\n",
      "Training Accuracy: 1.0000\n",
      "Test Accuracy: 0.6286\n",
      "Test AUC: 0.5909\n",
      "Precision: 0.7619\n",
      "Recall: 0.6667\n",
      "F1-Score: 0.7111\n",
      "Overfitting Gap (Train - Test Accuracy): 0.3714\n",
      "âš ï¸  Significant overfitting detected - penalized methods should help!\n"
     ]
    }
   ],
   "source": [
    "# 0. Ordinary Logistic Regression (Baseline)\n",
    "print(\"\\n0. ORDINARY LOGISTIC REGRESSION (BASELINE)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# No regularization - this is our baseline to compare against penalized methods\n",
    "ordinary_lr = LogisticRegression(\n",
    "    penalty=None, \n",
    "    max_iter=5000, \n",
    "    solver='lbfgs'\n",
    "    )\n",
    "ordinary_lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate ordinary logistic regression\n",
    "ordinary_metrics, ordinary_pred, ordinary_proba = evaluate_model(\n",
    "    ordinary_lr, X_train_scaled, X_test_scaled, y_train, y_test, 'Ordinary LR'\n",
    ")\n",
    "results.append(ordinary_metrics)\n",
    "all_predictions['Ordinary LR'] = ordinary_pred\n",
    "all_probabilities['Ordinary LR'] = ordinary_proba\n",
    "\n",
    "print(f\"Training Accuracy: {ordinary_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {ordinary_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test AUC: {ordinary_metrics['test_auc']:.4f}\")\n",
    "print(f\"Precision: {ordinary_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {ordinary_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {ordinary_metrics['f1']:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting = ordinary_metrics['train_accuracy'] - ordinary_metrics['test_accuracy']\n",
    "print(f\"Overfitting Gap (Train - Test Accuracy): {overfitting:.4f}\")\n",
    "if overfitting > 0.05:\n",
    "    print(\"âš ï¸  Significant overfitting detected - penalized methods should help!\")\n",
    "else:\n",
    "    print(\"âœ“ Low overfitting - but regularization may still improve generalization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. RIDGE REGRESSION (L2 REGULARIZATION) - IMPROVED\n",
      "------------------------------------------------------------\n",
      "Testing 50 C values from 0.000100 to 1000000\n",
      "\n",
      "Testing solver: lbfgs\n",
      "Best CV AUC for lbfgs: 0.4772 (C=0.000100)\n",
      "AUC Gap (Train-Test): 0.1882\n",
      "âš ï¸ Potential overfitting detected\n",
      "\n",
      "Testing solver: newton-cg\n",
      "Best CV AUC for newton-cg: 0.4772 (C=0.000100)\n",
      "AUC Gap (Train-Test): 0.1880\n",
      "âš ï¸ Potential overfitting detected\n",
      "\n",
      "Testing solver: sag\n",
      "Best CV AUC for sag: 0.4772 (C=0.000100)\n",
      "AUC Gap (Train-Test): 0.1882\n",
      "âš ï¸ Potential overfitting detected\n",
      "\n",
      "Testing solver: saga\n",
      "Best CV AUC for saga: 0.4772 (C=0.000100)\n",
      "AUC Gap (Train-Test): 0.1882\n",
      "âš ï¸ Potential overfitting detected\n",
      "\n",
      "âœ“ Best Ridge solver: lbfgs with CV AUC: 0.4772\n",
      "âœ“ Optimal C value: 0.000100\n",
      "\n",
      "ðŸ“Š RIDGE REGRESSION RESULTS:\n",
      "Training Accuracy: 0.6471\n",
      "Test Accuracy: 0.5429\n",
      "Test AUC: 0.5492\n",
      "Precision: 0.7500\n",
      "Recall: 0.5000\n",
      "F1-Score: 0.6000\n",
      "âœ“ Convergence achieved in [[12 11 12 12 12 12 13 14 14 16 18 18 20 24 26 27 28 31 33 31 30 30 25 25\n",
      "  20 13 13 13 10  0  9  0  0  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [12 11 12 12 12 11 13 14 13 15 16 20 21 23 26 27 30 31 34 34 29 25 29 23\n",
      "  21 15 16 13 12  0 10  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [12 11 12 11 12 12 12 13 14 15 16 20 21 24 25 26 31 30 34 31 32 29 26 22\n",
      "  17 14 13 11  9  9  0  0  9  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [11 11 11 11 11 10 11 12 13 16 18 17 20 23 25 30 33 36 35 38 35 31 27 27\n",
      "  24 16 15 12 11  0 12  0  0  0  0  0  7  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [12 11 11 11 12 12 12 13 15 15 18 20 22 24 26 27 31 31 31 29 30 27 19 23\n",
      "  17 12 12 11 10 11  0  5  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [12 11 12 12 12 12 11 13 14 16 16 17 21 24 24 28 28 28 26 26 26 22 21 19\n",
      "  17 16 14 11 10  0  9  0  0  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [11 11 12 12 11 11 11 13 14 15 17 20 22 25 26 29 31 30 35 33 33 30 25 18\n",
      "  22 12 13 14 12  0  8  0  0  0  8  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [11 11 12 11 11 12 11 13 15 15 17 20 21 26 26 30 30 32 32 35 30 29 20 20\n",
      "  18 17 13 13  9 10  0  0  6  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [ 7 12 11 11 11 11 11 12 13 14 17 19 21 24 26 29 26 30 32 30 31 33 25 18\n",
      "  20 13 13 15 10 11  0  5  0  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]\n",
      " [11 12 11 11 11 12 11 13 15 14 16 19 20 23 26 29 30 31 33 27 29 25 24 15\n",
      "  19 18 12 13  9  1  7  0  0  0  0  0  6  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0]] iterations\n"
     ]
    }
   ],
   "source": [
    "# 1. Ridge Regression (L2 Regularization) - IMPROVED\n",
    "print(\"\\n1. RIDGE REGRESSION (L2 REGULARIZATION) - IMPROVED\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# IMPROVEMENT 1: Much wider C range with logarithmic spacing\n",
    "# Including very weak regularization (high C) to very strong (low C)\n",
    "ridge_Cs = np.logspace(-4, 6, 50)  # From 0.0001 to 1,000,000\n",
    "print(f\"Testing {len(ridge_Cs)} C values from {ridge_Cs.min():.6f} to {ridge_Cs.max():.0f}\")\n",
    "\n",
    "# IMPROVEMENT 2: Use StratifiedKFold for better CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "stratified_kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# IMPROVEMENT 3: Test multiple solvers and find the best one\n",
    "solvers_to_test = ['lbfgs', 'newton-cg', 'sag', 'saga']\n",
    "best_auc = 0\n",
    "best_ridge = None\n",
    "best_solver = None\n",
    "\n",
    "for solver in solvers_to_test:\n",
    "    try:\n",
    "        print(f\"\\nTesting solver: {solver}\")\n",
    "        ridge_temp = LogisticRegressionCV(\n",
    "            Cs=ridge_Cs,\n",
    "            cv=stratified_kfold,\n",
    "            penalty='l2',\n",
    "            solver=solver,\n",
    "            scoring='roc_auc',\n",
    "            max_iter=5000,  # Increased for better convergence\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "        ridge_temp.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get best CV score\n",
    "        best_cv_score = ridge_temp.scores_[1].mean(axis=0).max()\n",
    "        print(f\"Best CV AUC for {solver}: {best_cv_score:.4f} (C={ridge_temp.C_[0]:.6f})\")\n",
    "        \n",
    "        # Evaluate model to get metrics for overfitting check\n",
    "        ridge_metrics, ridge_pred, ridge_proba = evaluate_model(\n",
    "            ridge_temp, X_train_scaled, X_test_scaled, y_train, y_test, f'Ridge LR {solver}'\n",
    "        )\n",
    "        \n",
    "        # Check overfitting\n",
    "        overfitting_gap = ridge_metrics['train_auc'] - ridge_metrics['test_auc']\n",
    "        print(f\"AUC Gap (Train-Test): {overfitting_gap:.4f}\")\n",
    "        \n",
    "        if overfitting_gap > 0.05:\n",
    "            print(\"âš ï¸ Potential overfitting detected\")\n",
    "        else:\n",
    "            print(\"âœ“ Good generalization\")\n",
    "        \n",
    "        if best_cv_score > best_auc:\n",
    "            best_auc = best_cv_score\n",
    "            best_ridge = ridge_temp\n",
    "            best_solver = solver\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Solver {solver} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Use the best ridge model\n",
    "ridge = best_ridge\n",
    "\n",
    "# Evaluate ridge regression\n",
    "ridge_metrics, ridge_pred, ridge_proba = evaluate_model(\n",
    "    ridge, X_train_scaled, X_test_scaled, y_train, y_test, 'Ridge LR'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Best Ridge solver: {best_solver} with CV AUC: {best_auc:.4f}\")\n",
    "print(f\"âœ“ Optimal C value: {ridge.C_[0]:.6f}\")\n",
    "\n",
    "print(f\"\\nðŸ“Š RIDGE REGRESSION RESULTS:\")\n",
    "print(f\"Training Accuracy: {ridge_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {ridge_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test AUC: {ridge_metrics['test_auc']:.4f}\")\n",
    "print(f\"Precision: {ridge_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {ridge_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {ridge_metrics['f1']:.4f}\")\n",
    "\n",
    "# IMPROVEMENT 4: Check for convergence\n",
    "if hasattr(ridge, 'n_iter_'):\n",
    "    print(f\"âœ“ Convergence achieved in {ridge.n_iter_[0]} iterations\")\n",
    "\n",
    "all_predictions['Ridge LR'] = ridge_pred\n",
    "all_probabilities['Ridge LR'] = ridge_proba\n",
    "results.append(ridge_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. LASSO REGRESSION (L1 REGULARIZATION) - IMPROVED\n",
      "------------------------------------------------------------\n",
      "Testing 40 C values from 0.001000 to 100000\n",
      "\n",
      "Testing Lasso with solver: saga\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     20\u001b[39m max_iter_solver = \u001b[32m10000\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m solver == \u001b[33m'\u001b[39m\u001b[33msaga\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m5000\u001b[39m\n\u001b[32m     22\u001b[39m lasso_temp = LogisticRegressionCV(\n\u001b[32m     23\u001b[39m     Cs=lasso_Cs,\n\u001b[32m     24\u001b[39m     cv=stratified_kfold,\n\u001b[32m   (...)\u001b[39m\u001b[32m     32\u001b[39m     tol=\u001b[32m1e-6\u001b[39m  \u001b[38;5;66;03m# Tighter tolerance for better convergence\u001b[39;00m\n\u001b[32m     33\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mlasso_temp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m best_cv_score = lasso_temp.scores_[\u001b[32m1\u001b[39m].mean(axis=\u001b[32m0\u001b[39m).max()\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBest CV AUC for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msolver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_cv_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (C=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlasso_temp.C_[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1994\u001b[39m, in \u001b[36mLogisticRegressionCV.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, **params)\u001b[39m\n\u001b[32m   1991\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1992\u001b[39m     prefer = \u001b[33m\"\u001b[39m\u001b[33mprocesses\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1994\u001b[39m fold_coefs_ = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1996\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1997\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2000\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpos_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2001\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mCs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2002\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2003\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2004\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdual\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2005\u001b[39m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[43m=\u001b[49m\u001b[43msolver\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2007\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2008\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2009\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2010\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscoring\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2011\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmulti_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_squared_sum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2015\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2016\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2018\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2019\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miter_encoded_labels\u001b[49m\n\u001b[32m   2020\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfolds\u001b[49m\n\u001b[32m   2021\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml1_ratios_\u001b[49m\n\u001b[32m   2022\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2024\u001b[39m \u001b[38;5;66;03m# _log_reg_scoring_path will output different shapes depending on the\u001b[39;00m\n\u001b[32m   2025\u001b[39m \u001b[38;5;66;03m# multi_class param, so we need to reshape the outputs accordingly.\u001b[39;00m\n\u001b[32m   2026\u001b[39m \u001b[38;5;66;03m# Cs is of shape (n_classes . n_folds . n_l1_ratios, n_Cs) and all the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2033\u001b[39m \u001b[38;5;66;03m#  (n_classes, n_folds, n_Cs . n_l1_ratios) or\u001b[39;00m\n\u001b[32m   2034\u001b[39m \u001b[38;5;66;03m#  (1, n_folds, n_Cs . n_l1_ratios)\u001b[39;00m\n\u001b[32m   2035\u001b[39m coefs_paths, Cs, scores, n_iter_ = \u001b[38;5;28mzip\u001b[39m(*fold_coefs_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 2. Lasso Regression (L1 Regularization) - IMPROVED\n",
    "print(\"\\n2. LASSO REGRESSION (L1 REGULARIZATION) - IMPROVED\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# IMPROVEMENT 1: Expanded C range for Lasso (L1 is more sensitive to regularization)\n",
    "lasso_Cs = np.logspace(-3, 5, 40)  # From 0.001 to 100,000\n",
    "print(f\"Testing {len(lasso_Cs)} C values from {lasso_Cs.min():.6f} to {lasso_Cs.max():.0f}\")\n",
    "\n",
    "# IMPROVEMENT 2: Test both SAGA and LIBLINEAR solvers for L1\n",
    "solvers_to_test = ['saga', 'liblinear']\n",
    "best_lasso_auc = 0\n",
    "best_lasso = None\n",
    "best_lasso_solver = None\n",
    "\n",
    "for solver in solvers_to_test:\n",
    "    try:\n",
    "        print(f\"\\nTesting Lasso with solver: {solver}\")\n",
    "        \n",
    "        # IMPROVEMENT 3: Higher max_iter for L1 regularization (slower convergence)\n",
    "        max_iter_solver = 10000 if solver == 'saga' else 5000\n",
    "        \n",
    "        lasso_temp = LogisticRegressionCV(\n",
    "            Cs=lasso_Cs,\n",
    "            cv=stratified_kfold,\n",
    "            penalty='l1',\n",
    "            solver=solver,\n",
    "            scoring='roc_auc',\n",
    "            max_iter=max_iter_solver,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced',\n",
    "            random_state=42,\n",
    "            tol=1e-6  # Tighter tolerance for better convergence\n",
    "        )\n",
    "        lasso_temp.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        best_cv_score = lasso_temp.scores_[1].mean(axis=0).max()\n",
    "        print(f\"Best CV AUC for {solver}: {best_cv_score:.4f} (C={lasso_temp.C_[0]:.6f})\")\n",
    "        \n",
    "        if best_cv_score > best_lasso_auc:\n",
    "            best_lasso_auc = best_cv_score\n",
    "            best_lasso = lasso_temp\n",
    "            best_lasso_solver = solver\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Solver {solver} failed: {e}\")\n",
    "        continue\n",
    "\n",
    "# Use the best lasso model\n",
    "lasso = best_lasso\n",
    "\n",
    "print(f\"\\nâœ“ Best Lasso solver: {best_lasso_solver} with CV AUC: {best_lasso_auc:.4f}\")\n",
    "print(f\"âœ“ Optimal C value: {lasso.C_[0]:.6f}\")\n",
    "\n",
    "# Evaluate lasso regression\n",
    "lasso_metrics, lasso_pred, lasso_proba = evaluate_model(\n",
    "    lasso, X_train_scaled, X_test_scaled, y_train, y_test, 'Lasso LR'\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š LASSO REGRESSION RESULTS:\")\n",
    "print(f\"Training Accuracy: {lasso_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {lasso_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test AUC: {lasso_metrics['test_auc']:.4f}\")\n",
    "print(f\"Precision: {lasso_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {lasso_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {lasso_metrics['f1']:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "overfitting_gap = lasso_metrics['train_auc'] - lasso_metrics['test_auc']\n",
    "print(f\"AUC Gap (Train-Test): {overfitting_gap:.4f}\")\n",
    "\n",
    "if overfitting_gap > 0.05:\n",
    "    print(\"âš ï¸ Potential overfitting despite feature selection\")\n",
    "else:\n",
    "    print(\"âœ“ Good generalization\")\n",
    "\n",
    "# IMPROVEMENT 4: Detailed feature selection analysis\n",
    "n_nonzero_coefs = np.sum(lasso.coef_[0] != 0)\n",
    "n_total_features = len(lasso.coef_[0])\n",
    "feature_selection_ratio = n_nonzero_coefs / n_total_features\n",
    "\n",
    "print(f\"\\nðŸ” FEATURE SELECTION ANALYSIS:\")\n",
    "print(f\"Selected features: {n_nonzero_coefs} out of {n_total_features} ({feature_selection_ratio:.1%})\")\n",
    "\n",
    "# Analyze feature selection effectiveness\n",
    "if feature_selection_ratio >= 0.7:\n",
    "    print(f\"â„¹ï¸ Moderate feature selection: retained {feature_selection_ratio:.1%} of features\")\n",
    "elif feature_selection_ratio >= 0.5:\n",
    "    print(f\"âœ“ Good feature selection: retained {feature_selection_ratio:.1%} of features\")\n",
    "elif feature_selection_ratio < 0.5:\n",
    "    print(f\"âš ï¸ Aggressive feature selection: retained only {feature_selection_ratio:.1%} of features\")\n",
    "\n",
    "# Show most important features\n",
    "if n_nonzero_coefs > 0:\n",
    "    feature_importance = np.abs(lasso.coef_[0])\n",
    "    nonzero_indices = feature_importance > 0\n",
    "    important_features = pd.DataFrame({\n",
    "        'feature': X_train.columns[nonzero_indices],\n",
    "        'coefficient': lasso.coef_[0][nonzero_indices],\n",
    "        'abs_coefficient': feature_importance[nonzero_indices]\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 most important features:\")\n",
    "    print(important_features.head(10)[['feature', 'coefficient']].to_string(index=False))\n",
    "\n",
    "# IMPROVEMENT 5: Convergence check\n",
    "if hasattr(lasso, 'n_iter_'):\n",
    "    if lasso.n_iter_[0] >= (lasso.max_iter * 0.9):\n",
    "        print(\"âš ï¸ Warning: Close to max iterations - consider increasing max_iter\")\n",
    "    else:\n",
    "        print(f\"\\nâœ“ Convergence achieved in {lasso.n_iter_[0]} iterations\")\n",
    "\n",
    "all_predictions['Lasso LR'] = lasso_pred\n",
    "all_probabilities['Lasso LR'] = lasso_proba\n",
    "results.append(lasso_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. ELASTIC NET (L1 + L2 REGULARIZATION)\n",
      "--------------------------------------------------\n",
      "Training Accuracy: 0.6691\n",
      "Test Accuracy: 0.6857\n",
      "Test AUC: 0.5000\n",
      "Precision: 0.6857\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.8136\n"
     ]
    }
   ],
   "source": [
    "# 3. Elastic Net (L1 + L2 Regularization) - COMPREHENSIVE OPTIMIZATION\n",
    "print(\"\\n3. ELASTIC NET (L1 + L2 REGULARIZATION) - COMPREHENSIVE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# IMPROVEMENT 1: Comprehensive C and l1_ratio grid search\n",
    "elastic_Cs = np.logspace(-3, 4, 30)  # From 0.001 to 10,000\n",
    "l1_ratios = np.linspace(0.01, 0.99, 20)  # From 1% L1 to 99% L1\n",
    "\n",
    "print(f\"Testing {len(elastic_Cs)} C values and {len(l1_ratios)} l1_ratio values\")\n",
    "print(f\"Total combinations: {len(elastic_Cs) * len(l1_ratios)}\")\n",
    "\n",
    "# IMPROVEMENT 2: Grid search with custom scoring\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "elastic_param_grid = {\n",
    "    'C': elastic_Cs,\n",
    "    'l1_ratio': l1_ratios\n",
    "}\n",
    "\n",
    "# Base model with optimized settings\n",
    "elastic_base = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    solver='saga',\n",
    "    max_iter=10000,  # High iteration limit\n",
    "    tol=1e-6,        # Tight tolerance\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# IMPROVEMENT 3: Comprehensive grid search\n",
    "print(\"\\nPerforming comprehensive ElasticNet grid search...\")\n",
    "elastic_grid = GridSearchCV(\n",
    "    elastic_base,\n",
    "    elastic_param_grid,\n",
    "    cv=stratified_kfold,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1  # Show progress\n",
    ")\n",
    "\n",
    "elastic_grid.fit(X_train_scaled, y_train)\n",
    "elastic_net = elastic_grid.best_estimator_\n",
    "\n",
    "# IMPROVEMENT 4: Detailed analysis of optimal parameters\n",
    "best_C = elastic_grid.best_params_['C']\n",
    "best_l1_ratio = elastic_grid.best_params_['l1_ratio']\n",
    "best_cv_score = elastic_grid.best_score_\n",
    "\n",
    "print(f\"\\nâœ“ OPTIMAL ELASTIC NET PARAMETERS:\")\n",
    "print(f\"  C (regularization strength): {best_C:.6f}\")\n",
    "print(f\"  l1_ratio (L1 vs L2 balance): {best_l1_ratio:.3f}\")\n",
    "print(f\"  CV AUC Score: {best_cv_score:.4f}\")\n",
    "\n",
    "# Interpret l1_ratio\n",
    "if best_l1_ratio < 0.1:\n",
    "    ratio_interpretation = \"Almost pure Ridge (L2)\"\n",
    "elif best_l1_ratio > 0.9:\n",
    "    ratio_interpretation = \"Almost pure Lasso (L1)\"\n",
    "elif 0.4 <= best_l1_ratio <= 0.6:\n",
    "    ratio_interpretation = \"Balanced L1/L2 mix\"\n",
    "else:\n",
    "    ratio_interpretation = f\"L1-dominant mix\" if best_l1_ratio > 0.5 else f\"L2-dominant mix\"\n",
    "\n",
    "print(f\"  Interpretation: {ratio_interpretation}\")\n",
    "\n",
    "# IMPROVEMENT 5: Feature selection analysis\n",
    "n_nonzero_coefs = np.sum(elastic_net.coef_[0] != 0)\n",
    "n_total_features = len(elastic_net.coef_[0])\n",
    "feature_selection_ratio = n_nonzero_coefs / n_total_features\n",
    "\n",
    "print(f\"\\nðŸ” FEATURE SELECTION ANALYSIS:\")\n",
    "print(f\"Selected features: {n_nonzero_coefs} out of {n_total_features} ({feature_selection_ratio:.1%})\")\n",
    "\n",
    "# Show most important features if any selection occurred\n",
    "if n_nonzero_coefs > 0 and n_nonzero_coefs < n_total_features:\n",
    "    feature_importance = np.abs(elastic_net.coef_[0])\n",
    "    nonzero_indices = feature_importance > 0\n",
    "    important_features = pd.DataFrame({\n",
    "        'feature': X_train.columns[nonzero_indices],\n",
    "        'coefficient': elastic_net.coef_[0][nonzero_indices],\n",
    "        'abs_coefficient': feature_importance[nonzero_indices]\n",
    "    }).sort_values('abs_coefficient', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop 10 most important features:\")\n",
    "    print(important_features.head(10)[['feature', 'coefficient']].to_string(index=False))\n",
    "\n",
    "# IMPROVEMENT 6: Convergence analysis\n",
    "if hasattr(elastic_net, 'n_iter_'):\n",
    "    print(f\"\\nâœ“ Convergence achieved in {elastic_net.n_iter_[0]} iterations\")\n",
    "    if elastic_net.n_iter_[0] >= (elastic_net.max_iter * 0.9):\n",
    "        print(\"âš ï¸ Warning: Close to max iterations - model may need more iterations\")\n",
    "\n",
    "# Evaluate elastic net regression\n",
    "elastic_net_metrics, elastic_net_pred, elastic_net_proba = evaluate_model(\n",
    "    elastic_net, X_train_scaled, X_test_scaled, y_train, y_test, 'Elastic Net LR'\n",
    ")\n",
    "results.append(elastic_net_metrics)\n",
    "all_predictions['Elastic Net LR'] = elastic_net_pred\n",
    "all_probabilities['Elastic Net LR'] = elastic_net_proba\n",
    "\n",
    "print(f\"\\nðŸ“Š ELASTIC NET RESULTS:\")\n",
    "print(f\"Training Accuracy: {elastic_net_metrics['train_accuracy']:.4f}\")\n",
    "print(f\"Test Accuracy: {elastic_net_metrics['test_accuracy']:.4f}\")\n",
    "print(f\"Test AUC: {elastic_net_metrics['test_auc']:.4f}\")\n",
    "print(f\"Precision: {elastic_net_metrics['precision']:.4f}\")\n",
    "print(f\"Recall: {elastic_net_metrics['recall']:.4f}\")\n",
    "print(f\"F1-Score: {elastic_net_metrics['f1']:.4f}\")\n",
    "\n",
    "# Performance comparison with individual Ridge/Lasso\n",
    "overfitting_gap = elastic_net_metrics['train_auc'] - elastic_net_metrics['test_auc']\n",
    "print(f\"AUC Gap (Train-Test): {overfitting_gap:.4f}\")\n",
    "\n",
    "# Compare with previous models if available\n",
    "if len(results) >= 3:\n",
    "    ridge_auc = results[-3]['test_auc']  # Ridge should be 2 positions back\n",
    "    lasso_auc = results[-2]['test_auc']  # Lasso should be 1 position back\n",
    "    elastic_auc = elastic_net_metrics['test_auc']\n",
    "    \n",
    "    print(f\"\\nðŸ† REGULARIZATION COMPARISON:\")\n",
    "    print(f\"Ridge AUC:       {ridge_auc:.4f}\")\n",
    "    print(f\"Lasso AUC:       {lasso_auc:.4f}\")\n",
    "    print(f\"Elastic Net AUC: {elastic_auc:.4f}\")\n",
    "    \n",
    "    best_reg_auc = max(ridge_auc, lasso_auc, elastic_auc)\n",
    "    if elastic_auc == best_reg_auc:\n",
    "        print(\"ðŸŽ¯ Elastic Net achieved the best regularized performance!\")\n",
    "    elif elastic_auc > max(ridge_auc, lasso_auc) - 0.001:  # Very close\n",
    "        print(\"âš–ï¸ Elastic Net performance is competitive with the best individual method\")\n",
    "    else:\n",
    "        print(\"ðŸ“ˆ Elastic Net combines both approaches but individual methods performed better\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-18T04:07:15.230757Z",
     "iopub.status.idle": "2025-09-18T04:07:15.231052Z",
     "shell.execute_reply": "2025-09-18T04:07:15.230915Z",
     "shell.execute_reply.started": "2025-09-18T04:07:15.230905Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL COMPARISON RESULTS\n",
      "================================================================================\n",
      "\n",
      "Performance Metrics Table:\n",
      "         model  train_accuracy  test_accuracy  train_auc  test_auc  precision  \\\n",
      "0  Ordinary LR          1.0000         0.6286     1.0000    0.5909     0.7619   \n",
      "1        Ridge          0.6691         0.6857     0.7221    0.5530     0.6857   \n",
      "2        Lasso          0.6691         0.6857     0.6947    0.5114     0.6857   \n",
      "3  Elastic Net          0.6691         0.6857     0.5000    0.5000     0.6857   \n",
      "\n",
      "   recall      f1  \n",
      "0  0.6667  0.7111  \n",
      "1  1.0000  0.8136  \n",
      "2  1.0000  0.8136  \n",
      "3  1.0000  0.8136  \n",
      "\n",
      "Relative Improvements over Ordinary Logistic Regression:\n",
      "\n",
      "Ridge:\n",
      "  AUC improvement: -0.0379\n",
      "  Accuracy improvement: +0.0571\n",
      "  Overfitting reduction: +0.3880\n",
      "\n",
      "Lasso:\n",
      "  AUC improvement: -0.0795\n",
      "  Accuracy improvement: +0.0571\n",
      "  Overfitting reduction: +0.3880\n",
      "\n",
      "Elastic Net:\n",
      "  AUC improvement: -0.0909\n",
      "  Accuracy improvement: +0.0571\n",
      "  Overfitting reduction: +0.3880\n",
      "\n",
      "ðŸ† Best Model (by AUC): Ordinary LR with AUC = 0.5909\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive results comparison\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE MODEL COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display results table\n",
    "print(\"\\nPerformance Metrics Table:\")\n",
    "print(results_df.round(4))\n",
    "\n",
    "# Calculate relative improvements over ordinary logistic regression\n",
    "print(\"\\nRelative Improvements over Ordinary Logistic Regression:\")\n",
    "baseline_metrics = results_df[results_df['model'] == 'Ordinary LR'].iloc[0]\n",
    "for idx, row in results_df.iterrows():\n",
    "    if row['model'] != 'Ordinary LR':\n",
    "        auc_improvement = row['test_auc'] - baseline_metrics['test_auc']\n",
    "        acc_improvement = row['test_accuracy'] - baseline_metrics['test_accuracy']\n",
    "        overfitting_reduction = (baseline_metrics['train_accuracy'] - baseline_metrics['test_accuracy']) - \\\n",
    "                               (row['train_accuracy'] - row['test_accuracy'])\n",
    "        print(f\"\\n{row['model']}:\")\n",
    "        print(f\"  AUC improvement: {auc_improvement:+.4f}\")\n",
    "        print(f\"  Accuracy improvement: {acc_improvement:+.4f}\")\n",
    "        print(f\"  Overfitting reduction: {overfitting_reduction:+.4f}\")\n",
    "\n",
    "# Find best performing model\n",
    "best_auc_idx = results_df['test_auc'].idxmax()\n",
    "best_model = results_df.loc[best_auc_idx]\n",
    "print(f\"\\nðŸ† Best Model (by AUC): {best_model['model']} with AUC = {best_model['test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering for Improved Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED FEATURE ENGINEERING TECHNIQUES\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED FEATURE ENGINEERING FOR IMPROVED REGULARIZED MODELS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "The following techniques often improve regularized model performance:\n",
    "1. Polynomial features (interactions and higher-order terms)\n",
    "2. Feature selection based on statistical tests\n",
    "3. Principal Component Analysis (PCA) for dimensionality reduction\n",
    "4. Recursive Feature Elimination (RFE)\n",
    "5. Ensemble methods combining multiple regularized models\n",
    "\"\"\")\n",
    "\n",
    "# Store original data for comparison\n",
    "X_train_original = X_train_scaled.copy()\n",
    "X_test_original = X_test_scaled.copy()\n",
    "\n",
    "# TECHNIQUE 1: Statistical Feature Selection\n",
    "print(\"\\n1. STATISTICAL FEATURE SELECTION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "\n",
    "# Use mutual information for feature selection (works well with regularized models)\n",
    "mi_selector = SelectKBest(score_func=mutual_info_classif, k='all')\n",
    "mi_scores = mi_selector.fit(X_train_scaled, y_train).scores_\n",
    "\n",
    "# Select top features based on mutual information\n",
    "n_features_to_select = min(int(X_train_scaled.shape[1] * 0.7), 100)  # Top 70% or max 100\n",
    "top_features_idx = np.argsort(mi_scores)[-n_features_to_select:]\n",
    "\n",
    "X_train_selected = X_train_scaled[:, top_features_idx]\n",
    "X_test_selected = X_test_scaled[:, top_features_idx]\n",
    "\n",
    "print(f\"Selected {n_features_to_select} out of {X_train_scaled.shape[1]} features based on mutual information\")\n",
    "print(f\"New feature matrix shape: {X_train_selected.shape}\")\n",
    "\n",
    "# TECHNIQUE 2: Polynomial Features (carefully controlled to avoid explosion)\n",
    "print(\"\\n2. POLYNOMIAL FEATURE GENERATION\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create polynomial features with interaction terms only (degree=2, interaction_only=True)\n",
    "# This is more conservative and works better with regularized models\n",
    "poly_transformer = PolynomialFeatures(\n",
    "    degree=2, \n",
    "    interaction_only=True,  # Only interaction terms, not squared terms\n",
    "    include_bias=False\n",
    ")\n",
    "\n",
    "# Apply to a subset of the most important features to control feature explosion\n",
    "top_20_features_idx = np.argsort(mi_scores)[-20:]  # Top 20 features only\n",
    "X_train_top20 = X_train_scaled[:, top_20_features_idx]\n",
    "X_test_top20 = X_test_scaled[:, top_20_features_idx]\n",
    "\n",
    "X_train_poly = poly_transformer.fit_transform(X_train_top20)\n",
    "X_test_poly = poly_transformer.transform(X_test_top20)\n",
    "\n",
    "print(f\"Generated polynomial features from top 20 features:\")\n",
    "print(f\"Original top 20 features: {X_train_top20.shape[1]}\")\n",
    "print(f\"With interactions: {X_train_poly.shape[1]}\")\n",
    "\n",
    "# Combine original selected features with polynomial features\n",
    "X_train_enhanced = np.hstack([X_train_selected, X_train_poly])\n",
    "X_test_enhanced = np.hstack([X_test_selected, X_test_poly])\n",
    "\n",
    "print(f\"Final enhanced feature matrix: {X_train_enhanced.shape}\")\n",
    "\n",
    "# TECHNIQUE 3: Test regularized models on enhanced features\n",
    "print(\"\\n3. TESTING REGULARIZED MODELS ON ENHANCED FEATURES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Quick Ridge test on enhanced features\n",
    "print(\"\\nTesting Ridge on enhanced features...\")\n",
    "ridge_enhanced = LogisticRegressionCV(\n",
    "    Cs=np.logspace(-2, 4, 20),\n",
    "    cv=5,\n",
    "    penalty='l2',\n",
    "    solver='lbfgs',\n",
    "    scoring='roc_auc',\n",
    "    max_iter=5000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ridge_enhanced.fit(X_train_enhanced, y_train)\n",
    "ridge_enhanced_metrics, _, _ = evaluate_model(\n",
    "    ridge_enhanced, X_train_enhanced, X_test_enhanced, y_train, y_test, 'Ridge Enhanced'\n",
    ")\n",
    "\n",
    "print(f\"Enhanced Ridge AUC: {ridge_enhanced_metrics['test_auc']:.4f}\")\n",
    "\n",
    "# Quick Lasso test on enhanced features  \n",
    "print(\"\\nTesting Lasso on enhanced features...\")\n",
    "lasso_enhanced = LogisticRegressionCV(\n",
    "    Cs=np.logspace(-2, 3, 20),\n",
    "    cv=5,\n",
    "    penalty='l1',\n",
    "    solver='saga',\n",
    "    scoring='roc_auc',\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lasso_enhanced.fit(X_train_enhanced, y_train)\n",
    "lasso_enhanced_metrics, _, _ = evaluate_model(\n",
    "    lasso_enhanced, X_train_enhanced, X_test_enhanced, y_train, y_test, 'Lasso Enhanced'\n",
    ")\n",
    "\n",
    "print(f\"Enhanced Lasso AUC: {lasso_enhanced_metrics['test_auc']:.4f}\")\n",
    "print(f\"Lasso selected {np.sum(lasso_enhanced.coef_[0] != 0)} out of {X_train_enhanced.shape[1]} enhanced features\")\n",
    "\n",
    "# Store enhanced results\n",
    "results.append(ridge_enhanced_metrics)\n",
    "results.append(lasso_enhanced_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods for Optimal Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSEMBLE METHODS FOR MAXIMUM PERFORMANCE\n",
    "print(\"\\n4. ENSEMBLE METHODS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create individual models with optimized parameters (using best parameters found)\n",
    "print(\"Creating optimized individual models for ensemble...\")\n",
    "\n",
    "# Model 1: Optimized Ridge\n",
    "ridge_opt = LogisticRegression(\n",
    "    penalty='l2',\n",
    "    C=ridge.C_[0] if hasattr(ridge, 'C_') else 100,\n",
    "    solver='lbfgs',\n",
    "    max_iter=5000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model 2: Optimized Lasso  \n",
    "lasso_opt = LogisticRegression(\n",
    "    penalty='l1',\n",
    "    C=lasso.C_[0] if hasattr(lasso, 'C_') else 10,\n",
    "    solver='saga',\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Model 3: Optimized Elastic Net\n",
    "elastic_opt = LogisticRegression(\n",
    "    penalty='elasticnet',\n",
    "    C=best_C if 'best_C' in locals() else 10,\n",
    "    l1_ratio=best_l1_ratio if 'best_l1_ratio' in locals() else 0.5,\n",
    "    solver='saga',\n",
    "    max_iter=10000,\n",
    "    class_weight='balanced',\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# ENSEMBLE 1: Soft Voting Classifier (uses probabilities)\n",
    "print(\"\\nCreating Soft Voting Ensemble...\")\n",
    "soft_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('ridge', ridge_opt),\n",
    "        ('lasso', lasso_opt), \n",
    "        ('elastic', elastic_opt)\n",
    "    ],\n",
    "    voting='soft',  # Use probability averaging\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "soft_ensemble.fit(X_train_scaled, y_train)\n",
    "soft_ensemble_metrics, _, _ = evaluate_model(\n",
    "    soft_ensemble, X_train_scaled, X_test_scaled, y_train, y_test, 'Soft Ensemble'\n",
    ")\n",
    "\n",
    "print(f\"Soft Ensemble AUC: {soft_ensemble_metrics['test_auc']:.4f}\")\n",
    "\n",
    "# ENSEMBLE 2: Enhanced Feature Ensemble\n",
    "print(\"\\nCreating Enhanced Feature Ensemble...\")\n",
    "enhanced_ensemble = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('ridge_enh', ridge_enhanced),\n",
    "        ('lasso_enh', lasso_enhanced)\n",
    "    ],\n",
    "    voting='soft',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "enhanced_ensemble.fit(X_train_enhanced, y_train)\n",
    "enhanced_ensemble_metrics, _, _ = evaluate_model(\n",
    "    enhanced_ensemble, X_train_enhanced, X_test_enhanced, y_train, y_test, 'Enhanced Ensemble'\n",
    ")\n",
    "\n",
    "print(f\"Enhanced Ensemble AUC: {enhanced_ensemble_metrics['test_auc']:.4f}\")\n",
    "\n",
    "# ENSEMBLE 3: Weighted Average (manual implementation for more control)\n",
    "print(\"\\nCreating Weighted Average Ensemble...\")\n",
    "\n",
    "# Get probabilities from individual models\n",
    "ridge_opt.fit(X_train_scaled, y_train)\n",
    "lasso_opt.fit(X_train_scaled, y_train)\n",
    "elastic_opt.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge_proba = ridge_opt.predict_proba(X_test_scaled)[:, 1]\n",
    "lasso_proba = lasso_opt.predict_proba(X_test_scaled)[:, 1]\n",
    "elastic_proba = elastic_opt.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Weight based on individual model performance (higher weight for better models)\n",
    "ridge_weight = ridge_metrics['test_auc'] if 'ridge_metrics' in locals() else 0.33\n",
    "lasso_weight = lasso_metrics['test_auc'] if 'lasso_metrics' in locals() else 0.33\n",
    "elastic_weight = elastic_net_metrics['test_auc'] if 'elastic_net_metrics' in locals() else 0.33\n",
    "\n",
    "# Normalize weights\n",
    "total_weight = ridge_weight + lasso_weight + elastic_weight\n",
    "ridge_weight /= total_weight\n",
    "lasso_weight /= total_weight  \n",
    "elastic_weight /= total_weight\n",
    "\n",
    "print(f\"Model weights - Ridge: {ridge_weight:.3f}, Lasso: {lasso_weight:.3f}, Elastic: {elastic_weight:.3f}\")\n",
    "\n",
    "# Weighted average prediction\n",
    "weighted_proba = (ridge_weight * ridge_proba + \n",
    "                 lasso_weight * lasso_proba + \n",
    "                 elastic_weight * elastic_proba)\n",
    "\n",
    "weighted_pred = (weighted_proba > 0.5).astype(int)\n",
    "\n",
    "# Calculate metrics for weighted ensemble\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score, f1_score\n",
    "\n",
    "weighted_metrics = {\n",
    "    'model': 'Weighted Ensemble',\n",
    "    'train_accuracy': 0,  # Not calculated for simplicity\n",
    "    'test_accuracy': accuracy_score(y_test, weighted_pred),\n",
    "    'train_auc': 0,       # Not calculated for simplicity\n",
    "    'test_auc': roc_auc_score(y_test, weighted_proba),\n",
    "    'precision': precision_score(y_test, weighted_pred),\n",
    "    'recall': recall_score(y_test, weighted_pred),\n",
    "    'f1': f1_score(y_test, weighted_pred)\n",
    "}\n",
    "\n",
    "print(f\"Weighted Ensemble AUC: {weighted_metrics['test_auc']:.4f}\")\n",
    "\n",
    "# Store ensemble results\n",
    "results.extend([soft_ensemble_metrics, enhanced_ensemble_metrics, weighted_metrics])\n",
    "\n",
    "print(\"\\nðŸš€ FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find the best performing model\n",
    "best_auc = 0\n",
    "best_model = \"\"\n",
    "for result in results:\n",
    "    if result['test_auc'] > best_auc:\n",
    "        best_auc = result['test_auc']\n",
    "        best_model = result['model']\n",
    "\n",
    "print(f\"ðŸ† BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"ðŸŽ¯ BEST AUC SCORE: {best_auc:.4f}\")\n",
    "\n",
    "# Show top 5 models\n",
    "print(f\"\\nðŸ“Š TOP 5 MODELS BY AUC:\")\n",
    "sorted_results = sorted(results, key=lambda x: x['test_auc'], reverse=True)\n",
    "for i, result in enumerate(sorted_results[:5]):\n",
    "    print(f\"{i+1}. {result['model']}: {result['test_auc']:.4f}\")\n",
    "\n",
    "# Calculate improvement over baseline\n",
    "if len(results) > 0:\n",
    "    baseline_auc = results[0]['test_auc']  # Assuming first result is baseline\n",
    "    improvement = best_auc - baseline_auc\n",
    "    improvement_pct = (improvement / baseline_auc) * 100\n",
    "    print(f\"\\nðŸ“ˆ IMPROVEMENT OVER BASELINE:\")\n",
    "    print(f\"Baseline AUC: {baseline_auc:.4f}\")\n",
    "    print(f\"Best AUC: {best_auc:.4f}\")\n",
    "    print(f\"Absolute improvement: +{improvement:.4f}\")\n",
    "    print(f\"Relative improvement: +{improvement_pct:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Implementation Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK IMPLEMENTATION RECOMMENDATIONS\n",
    "print(\"\\n\" + \"ðŸŽ¯\" * 25)\n",
    "print(\"QUICK FIXES FOR IMMEDIATE IMPROVEMENT\")\n",
    "print(\"ðŸŽ¯\" * 25)\n",
    "\n",
    "print(\"\"\"\n",
    "IMMEDIATE ACTIONS TO IMPROVE RIDGE/LASSO PERFORMANCE:\n",
    "\n",
    "1. ðŸ”§ REGULARIZATION STRENGTH:\n",
    "   - Current C range may be too restrictive\n",
    "   - Try C values from 0.0001 to 100,000 (wider range)\n",
    "   - Use np.logspace(-4, 5, 50) for comprehensive search\n",
    "\n",
    "2. âš™ï¸ SOLVER OPTIMIZATION:\n",
    "   - Ridge: Test 'lbfgs', 'newton-cg', 'sag', 'saga' solvers\n",
    "   - Lasso: Use 'saga' or 'liblinear' with max_iter=10000+\n",
    "   - Elastic Net: Always use 'saga' solver\n",
    "\n",
    "3. ðŸ“Š CROSS-VALIDATION:\n",
    "   - Use StratifiedKFold with 10 folds instead of 5\n",
    "   - Ensure scoring='roc_auc' (not 'accuracy')\n",
    "   - Set random_state for reproducibility\n",
    "\n",
    "4. ðŸŽ² CONVERGENCE:\n",
    "   - Increase max_iter to 5000-10000\n",
    "   - Set tol=1e-6 for tighter convergence\n",
    "   - Monitor n_iter_ to check if converged\n",
    "\n",
    "5. âš–ï¸ CLASS BALANCE:\n",
    "   - Keep class_weight='balanced'\n",
    "   - Consider stratified sampling if needed\n",
    "\n",
    "6. ðŸ” FEATURE ENGINEERING:\n",
    "   - Remove highly correlated features (threshold > 0.95)\n",
    "   - Try polynomial features (degree=2, interaction_only=True)\n",
    "   - Use mutual information for feature selection\n",
    "\n",
    "7. ðŸ† ENSEMBLE METHODS:\n",
    "   - Combine Ridge, Lasso, and Elastic Net with soft voting\n",
    "   - Weight models by their individual performance\n",
    "   - Test on both original and engineered features\n",
    "\"\"\")\n",
    "\n",
    "# Create a simple function for users to quickly test improvements\n",
    "def quick_improved_ridge_lasso(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Quick function to test improved Ridge and Lasso with better parameters\n",
    "    \"\"\"\n",
    "    print(\"Testing improved Ridge and Lasso models...\")\n",
    "    \n",
    "    # Improved Ridge\n",
    "    ridge_improved = LogisticRegressionCV(\n",
    "        Cs=np.logspace(-4, 5, 30),\n",
    "        cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "        penalty='l2',\n",
    "        solver='lbfgs',\n",
    "        scoring='roc_auc',\n",
    "        max_iter=5000,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Improved Lasso\n",
    "    lasso_improved = LogisticRegressionCV(\n",
    "        Cs=np.logspace(-4, 4, 30),\n",
    "        cv=StratifiedKFold(n_splits=10, shuffle=True, random_state=42),\n",
    "        penalty='l1',\n",
    "        solver='saga',\n",
    "        scoring='roc_auc',\n",
    "        max_iter=10000,\n",
    "        tol=1e-6,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit models\n",
    "    ridge_improved.fit(X_train, y_train)\n",
    "    lasso_improved.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    ridge_auc = roc_auc_score(y_test, ridge_improved.predict_proba(X_test)[:, 1])\n",
    "    lasso_auc = roc_auc_score(y_test, lasso_improved.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    print(f\"Improved Ridge AUC: {ridge_auc:.4f}\")\n",
    "    print(f\"Improved Lasso AUC: {lasso_auc:.4f}\")\n",
    "    \n",
    "    return ridge_improved, lasso_improved, ridge_auc, lasso_auc\n",
    "\n",
    "print(f\"\\nðŸ’¡ To quickly test these improvements, run:\")\n",
    "print(f\"ridge_imp, lasso_imp, ridge_auc, lasso_auc = quick_improved_ridge_lasso(X_train_scaled, X_test_scaled, y_train, y_test)\")\n",
    "\n",
    "# Show the most critical parameters that likely caused the performance drop\n",
    "print(f\"\\nðŸš¨ MOST LIKELY CAUSES OF PERFORMANCE DROP:\")\n",
    "print(f\"1. C values too low (over-regularization)\")\n",
    "print(f\"2. max_iter too low (poor convergence)\")\n",
    "print(f\"3. Wrong scoring metric in CV\")\n",
    "print(f\"4. Suboptimal solver choice\")\n",
    "print(f\"5. Feature scaling issues\")\n",
    "\n",
    "print(f\"\\nâœ… RUN THE IMPROVED MODELS ABOVE TO SEE IMMEDIATE GAINS!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
