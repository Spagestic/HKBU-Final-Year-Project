{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13195792,"sourceType":"datasetVersion","datasetId":8362584}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"0e1f368d-ac24-44a9-ba9a-6f038b857511","cell_type":"markdown","source":"Toxicity Dataset : https://archive.ics.uci.edu/dataset/728/toxicity-2\n\nThe dataset includes 171 molecules designed for functional domains of a core clock protein, CRY1, responsible for generating circadian rhythm. 56 of the molecules are toxic and the rest are non-toxic. \n\nThe data consists a complete set of 1203 molecular descriptors and needs feature selection before classification since some of the features are redundant. \n\nIntroductory Paper:\nStructure-based design and classifications of small molecules regulating the circadian rhythm period\nBy Seref Gul, F. Rahim, Safak Isin, Fatma Yilmaz, Nuri Ozturk, M. Turkay, I. Kavakli. 2021\nhttps://www.semanticscholar.org/paper/Structure-based-design-and-classifications-of-small-Gul-Rahim/5944836c47bc7d1a2b0464a9a1db94d4bc7f28ce","metadata":{}},{"id":"784a0694","cell_type":"code","source":"# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier, SGDClassifier\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.base import clone\nimport warnings\nwarnings.filterwarnings('ignore')\nimport time","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:07.490896Z","iopub.execute_input":"2025-11-04T04:00:07.491353Z","iopub.status.idle":"2025-11-04T04:00:09.332188Z","shell.execute_reply.started":"2025-11-04T04:00:07.491319Z","shell.execute_reply":"2025-11-04T04:00:09.331256Z"}},"outputs":[],"execution_count":1},{"id":"4f67d9b8","cell_type":"code","source":"# Load and preprocess data\ntry:\n    data = pd.read_csv(\"/kaggle/input/toxicity/data.csv\")\nexcept FileNotFoundError:\n    print(\"Trying './data.csv'\")\n    # A common alternative path structure\n    data = pd.read_csv(\"data.csv\")\n\n\nX = data.drop('Class', axis=1) if 'Class' in data.columns else data.iloc[:, :-1]\ny = data['Class'] if 'Class' in data.columns else data.iloc[:, -1]\ny_binary = (y == 'NonToxic').astype(int)\n\n\nprint(f\"Dataset shape: {data.shape}\")\nprint(f\"Class distribution:\\n{y.value_counts()}\")\nprint(f\"Class balance:\\n{y.value_counts(normalize=True)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.333635Z","iopub.execute_input":"2025-11-04T04:00:09.334104Z","iopub.status.idle":"2025-11-04T04:00:09.454561Z","shell.execute_reply.started":"2025-11-04T04:00:09.334070Z","shell.execute_reply":"2025-11-04T04:00:09.453552Z"}},"outputs":[{"name":"stdout","text":"Dataset shape: (171, 1204)\nClass distribution:\nClass\nNonToxic    115\nToxic        56\nName: count, dtype: int64\nClass balance:\nClass\nNonToxic    0.672515\nToxic       0.327485\nName: proportion, dtype: float64\n","output_type":"stream"}],"execution_count":2},{"id":"a982a9bb","cell_type":"code","source":"# Shuffle and split\nnp.random.seed(42)\nshuffle_idx = np.random.permutation(len(X))\nX_shuffled, y_shuffled = X.iloc[shuffle_idx].reset_index(drop=True), y_binary.iloc[shuffle_idx].reset_index(drop=True)\nX_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=42, stratify=y_shuffled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.455355Z","iopub.execute_input":"2025-11-04T04:00:09.455682Z","iopub.status.idle":"2025-11-04T04:00:09.472382Z","shell.execute_reply.started":"2025-11-04T04:00:09.455652Z","shell.execute_reply":"2025-11-04T04:00:09.471277Z"}},"outputs":[],"execution_count":3},{"id":"3bdded5e","cell_type":"code","source":"# Standardize\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nprint(f\"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.474317Z","iopub.execute_input":"2025-11-04T04:00:09.474735Z","iopub.status.idle":"2025-11-04T04:00:09.542789Z","shell.execute_reply.started":"2025-11-04T04:00:09.474705Z","shell.execute_reply":"2025-11-04T04:00:09.541671Z"}},"outputs":[{"name":"stdout","text":"Training set: (136, 1203), Test set: (35, 1203)\n","output_type":"stream"}],"execution_count":4},{"id":"75ccabd5","cell_type":"code","source":"# Define models to compare\nmodels = {\n    # === LINEAR MODELS ===\n    'LR_No_Penalty': LogisticRegression(penalty=None, max_iter=5000, solver='lbfgs'),\n    'LR_Ridge_C1': LogisticRegression(penalty='l2', C=1.0, max_iter=5000, solver='lbfgs'),\n    'LR_Ridge_C0.1': LogisticRegression(penalty='l2', C=0.1, max_iter=5000, solver='lbfgs'),\n    'LR_Ridge_C10': LogisticRegression(penalty='l2', C=10.0, max_iter=5000, solver='lbfgs'),\n    'LR_Lasso_C1': LogisticRegression(penalty='l1', C=1.0, max_iter=10000, solver='saga'), \n    'LR_Lasso_C0.1': LogisticRegression(penalty='l1', C=0.1, max_iter=10000, solver='saga'),\n    'LR_ElasticNet_L1_0.5': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=10000),\n    'LR_ElasticNet_L1_0.7': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.7, C=1.0, max_iter=10000),\n    'Ridge_Classifier': RidgeClassifier(alpha=1.0),\n    'SGD_Classifier': SGDClassifier(loss='log_loss', max_iter=5000, random_state=42),\n    \n    # === DISCRIMINANT ANALYSIS ===\n    'LDA': LinearDiscriminantAnalysis(),\n    'QDA': QuadraticDiscriminantAnalysis(),\n    \n    # === NAIVE BAYES ===\n    'Naive_Bayes': GaussianNB(),\n    \n    # === TREE-BASED MODELS ===\n    'Decision_Tree_D5': DecisionTreeClassifier(max_depth=5, random_state=42),\n    'Decision_Tree_D10': DecisionTreeClassifier(max_depth=10, random_state=42),\n    'Decision_Tree_D20': DecisionTreeClassifier(max_depth=20, random_state=42),\n    'Decision_Tree_Unpruned': DecisionTreeClassifier(random_state=42),\n    \n    # === ENSEMBLE MODELS - BAGGING ===\n    'Random_Forest_N50': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42),\n    'Random_Forest_N100': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42),\n    'Random_Forest_N200': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42),\n    'Random_Forest_Deep': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42),\n    'Extra_Trees_N100': ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42),\n    \n    # === ENSEMBLE MODELS - BOOSTING ===\n    'AdaBoost_N50': AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME'),\n    'AdaBoost_N100': AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME'),\n    'GradientBoosting_N50': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n    'GradientBoosting_N100': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),\n    'XGBoost_D3_N50': XGBClassifier(max_depth=3, n_estimators=50, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    'XGBoost_D3_N100': XGBClassifier(max_depth=3, n_estimators=100, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    'XGBoost_D5_N100': XGBClassifier(max_depth=5, n_estimators=100, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    \n    # === SVM VARIATIONS ===\n    'SVM_Linear': SVC(kernel='linear', probability=True, random_state=42),\n    'SVM_RBF_C1': SVC(kernel='rbf', C=1.0, probability=True, random_state=42),\n    'SVM_RBF_C10': SVC(kernel='rbf', C=10.0, probability=True, random_state=42),\n    'SVM_Poly_D2': SVC(kernel='poly', degree=2, probability=True, random_state=42),\n    'SVM_Poly_D3': SVC(kernel='poly', degree=3, probability=True, random_state=42),\n    \n    # === K-NEAREST NEIGHBORS ===\n    'KNN_K3': KNeighborsClassifier(n_neighbors=3),\n    'KNN_K5': KNeighborsClassifier(n_neighbors=5),\n    'KNN_K7': KNeighborsClassifier(n_neighbors=7),\n    'KNN_K10': KNeighborsClassifier(n_neighbors=10),\n    \n    # === NEURAL NETWORKS ===\n    'NN_Small': MLPClassifier(hidden_layer_sizes=(25,), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Medium': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Large': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Deep': MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Adam': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=42, early_stopping=True, solver='adam'),\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.543972Z","iopub.execute_input":"2025-11-04T04:00:09.544383Z","iopub.status.idle":"2025-11-04T04:00:09.559411Z","shell.execute_reply.started":"2025-11-04T04:00:09.544357Z","shell.execute_reply":"2025-11-04T04:00:09.558473Z"}},"outputs":[],"execution_count":5},{"id":"9c7e3b93","cell_type":"code","source":"# Evaluation function\ndef evaluate_model(model, X_train, X_test, y_train, y_test):\n    \"\"\"Evaluate a classification model and return metrics\"\"\"\n    X_train_selected, X_test_selected = X_train, X_test\n    model.fit(X_train_selected, y_train)\n    \n    # Predictions\n    y_train_pred = model.predict(X_train_selected)\n    y_test_pred = model.predict(X_test_selected)\n    \n    # Probabilities\n    if hasattr(model, 'predict_proba'):\n        y_train_proba = model.predict_proba(X_train_selected)[:, 1]\n        y_test_proba = model.predict_proba(X_test_selected)[:, 1]\n    else:\n        y_train_proba = model.decision_function(X_train_selected)\n        y_test_proba = model.decision_function(X_test_selected)\n    \n    return {\n        'train_acc': accuracy_score(y_train, y_train_pred),\n        'test_acc': accuracy_score(y_test, y_test_pred),\n        'train_auc': roc_auc_score(y_train, y_train_proba),\n        'test_auc': roc_auc_score(y_test, y_test_proba),\n        'precision': precision_score(y_test, y_test_pred),\n        'recall': recall_score(y_test, y_test_pred),\n        'f1': f1_score(y_test, y_test_pred)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.560456Z","iopub.execute_input":"2025-11-04T04:00:09.560739Z","iopub.status.idle":"2025-11-04T04:00:09.583602Z","shell.execute_reply.started":"2025-11-04T04:00:09.560717Z","shell.execute_reply":"2025-11-04T04:00:09.582534Z"}},"outputs":[],"execution_count":6},{"id":"104e44f5","cell_type":"code","source":"# Train and evaluate all models (WITHOUT class weights)\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING MODELS WITHOUT CLASS WEIGHTS\")\nprint(\"=\"*80)\nresults = []\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    metrics = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n    metrics['model'] = name\n    results.append(metrics)\n    print(f\"  Test Accuracy: {metrics['test_acc']:.4f}, Test AUC: {metrics['test_auc']:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:09.584666Z","iopub.execute_input":"2025-11-04T04:00:09.585696Z","iopub.status.idle":"2025-11-04T04:00:59.374233Z","shell.execute_reply.started":"2025-11-04T04:00:09.585657Z","shell.execute_reply":"2025-11-04T04:00:59.373246Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTRAINING MODELS WITHOUT CLASS WEIGHTS\n================================================================================\nTraining LR_No_Penalty...\n  Test Accuracy: 0.5714, Test AUC: 0.5871\nTraining LR_Ridge_C1...\n  Test Accuracy: 0.5714, Test AUC: 0.5644\nTraining LR_Ridge_C0.1...\n  Test Accuracy: 0.5714, Test AUC: 0.5758\nTraining LR_Ridge_C10...\n  Test Accuracy: 0.5714, Test AUC: 0.5606\nTraining LR_Lasso_C1...\n  Test Accuracy: 0.6000, Test AUC: 0.5455\nTraining LR_Lasso_C0.1...\n  Test Accuracy: 0.6857, Test AUC: 0.5644\nTraining LR_ElasticNet_L1_0.5...\n  Test Accuracy: 0.6000, Test AUC: 0.5417\nTraining LR_ElasticNet_L1_0.7...\n  Test Accuracy: 0.6000, Test AUC: 0.5455\nTraining Ridge_Classifier...\n  Test Accuracy: 0.5143, Test AUC: 0.5492\nTraining SGD_Classifier...\n  Test Accuracy: 0.5714, Test AUC: 0.4924\nTraining LDA...\n  Test Accuracy: 0.6000, Test AUC: 0.5473\nTraining QDA...\n  Test Accuracy: 0.5143, Test AUC: 0.4981\nTraining Naive_Bayes...\n  Test Accuracy: 0.4286, Test AUC: 0.5076\nTraining Decision_Tree_D5...\n  Test Accuracy: 0.6286, Test AUC: 0.5795\nTraining Decision_Tree_D10...\n  Test Accuracy: 0.6000, Test AUC: 0.5852\nTraining Decision_Tree_D20...\n  Test Accuracy: 0.6000, Test AUC: 0.5852\nTraining Decision_Tree_Unpruned...\n  Test Accuracy: 0.6000, Test AUC: 0.5852\nTraining Random_Forest_N50...\n  Test Accuracy: 0.6571, Test AUC: 0.6383\nTraining Random_Forest_N100...\n  Test Accuracy: 0.6286, Test AUC: 0.6212\nTraining Random_Forest_N200...\n  Test Accuracy: 0.6286, Test AUC: 0.6136\nTraining Random_Forest_Deep...\n  Test Accuracy: 0.6286, Test AUC: 0.6307\nTraining Extra_Trees_N100...\n  Test Accuracy: 0.6286, Test AUC: 0.6629\nTraining AdaBoost_N50...\n  Test Accuracy: 0.6286, Test AUC: 0.6364\nTraining AdaBoost_N100...\n  Test Accuracy: 0.6286, Test AUC: 0.5985\nTraining GradientBoosting_N50...\n  Test Accuracy: 0.6571, Test AUC: 0.6856\nTraining GradientBoosting_N100...\n  Test Accuracy: 0.6571, Test AUC: 0.6250\nTraining XGBoost_D3_N50...\n  Test Accuracy: 0.6000, Test AUC: 0.6553\nTraining XGBoost_D3_N100...\n  Test Accuracy: 0.6571, Test AUC: 0.6667\nTraining XGBoost_D5_N100...\n  Test Accuracy: 0.5714, Test AUC: 0.5720\nTraining SVM_Linear...\n  Test Accuracy: 0.5429, Test AUC: 0.4432\nTraining SVM_RBF_C1...\n  Test Accuracy: 0.6857, Test AUC: 0.3826\nTraining SVM_RBF_C10...\n  Test Accuracy: 0.4857, Test AUC: 0.4053\nTraining SVM_Poly_D2...\n  Test Accuracy: 0.6857, Test AUC: 0.3826\nTraining SVM_Poly_D3...\n  Test Accuracy: 0.6857, Test AUC: 0.3333\nTraining KNN_K3...\n  Test Accuracy: 0.6571, Test AUC: 0.6951\nTraining KNN_K5...\n  Test Accuracy: 0.6571, Test AUC: 0.7027\nTraining KNN_K7...\n  Test Accuracy: 0.6571, Test AUC: 0.6667\nTraining KNN_K10...\n  Test Accuracy: 0.6571, Test AUC: 0.6553\nTraining NN_Small...\n  Test Accuracy: 0.6000, Test AUC: 0.6496\nTraining NN_Medium...\n  Test Accuracy: 0.6286, Test AUC: 0.5360\nTraining NN_Large...\n  Test Accuracy: 0.5429, Test AUC: 0.5492\nTraining NN_Deep...\n  Test Accuracy: 0.5429, Test AUC: 0.5625\nTraining NN_Adam...\n  Test Accuracy: 0.5429, Test AUC: 0.4962\n","output_type":"stream"}],"execution_count":7},{"id":"aef92fa2","cell_type":"code","source":"# Create results DataFrame\nresults_df = pd.DataFrame(results)\nresults_df = results_df[['model', 'train_acc', 'test_acc', 'train_auc', 'test_auc', 'precision', 'recall', 'f1']]\nresults_df = results_df.sort_values('test_acc', ascending=False).reset_index(drop=True)\n\nprint(\"\\n=== MODEL COMPARISON (NO CLASS WEIGHTS) ===\")\nprint(results_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:59.375223Z","iopub.execute_input":"2025-11-04T04:00:59.375588Z","iopub.status.idle":"2025-11-04T04:00:59.393682Z","shell.execute_reply.started":"2025-11-04T04:00:59.375555Z","shell.execute_reply":"2025-11-04T04:00:59.392885Z"}},"outputs":[{"name":"stdout","text":"\n=== MODEL COMPARISON (NO CLASS WEIGHTS) ===\n                 model  train_acc  test_acc  train_auc  test_auc  precision   recall       f1\n           SVM_Poly_D2   0.720588  0.685714   0.011966  0.382576   0.685714 1.000000 0.813559\n            SVM_RBF_C1   0.727941  0.685714   0.010501  0.382576   0.685714 1.000000 0.813559\n         LR_Lasso_C0.1   0.669118  0.685714   0.723077  0.564394   0.685714 1.000000 0.813559\n           SVM_Poly_D3   0.720588  0.685714   0.006105  0.333333   0.685714 1.000000 0.813559\n     Random_Forest_N50   1.000000  0.657143   1.000000  0.638258   0.714286 0.833333 0.769231\n                KNN_K3   0.757353  0.657143   0.779487  0.695076   0.750000 0.750000 0.750000\n                KNN_K5   0.713235  0.657143   0.757021  0.702652   0.714286 0.833333 0.769231\n                KNN_K7   0.727941  0.657143   0.728816  0.666667   0.714286 0.833333 0.769231\n               KNN_K10   0.654412  0.657143   0.682051  0.655303   0.730769 0.791667 0.760000\n  GradientBoosting_N50   1.000000  0.657143   1.000000  0.685606   0.730769 0.791667 0.760000\n GradientBoosting_N100   1.000000  0.657143   1.000000  0.625000   0.730769 0.791667 0.760000\n       XGBoost_D3_N100   1.000000  0.657143   1.000000  0.666667   0.714286 0.833333 0.769231\n          AdaBoost_N50   1.000000  0.628571   1.000000  0.636364   0.720000 0.750000 0.734694\n         AdaBoost_N100   1.000000  0.628571   1.000000  0.598485   0.720000 0.750000 0.734694\n    Random_Forest_Deep   1.000000  0.628571   1.000000  0.630682   0.703704 0.791667 0.745098\n    Random_Forest_N200   1.000000  0.628571   1.000000  0.613636   0.703704 0.791667 0.745098\n    Random_Forest_N100   1.000000  0.628571   1.000000  0.621212   0.703704 0.791667 0.745098\n      Extra_Trees_N100   1.000000  0.628571   1.000000  0.662879   0.703704 0.791667 0.745098\n      Decision_Tree_D5   0.948529  0.628571   0.989866  0.579545   0.761905 0.666667 0.711111\n             NN_Medium   1.000000  0.628571   1.000000  0.535985   0.739130 0.708333 0.723404\nDecision_Tree_Unpruned   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n     Decision_Tree_D20   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n                   LDA   0.955882  0.600000   0.993407  0.547348   0.750000 0.625000 0.681818\n  LR_ElasticNet_L1_0.7   0.992647  0.600000   1.000000  0.545455   0.708333 0.708333 0.708333\n  LR_ElasticNet_L1_0.5   1.000000  0.600000   1.000000  0.541667   0.708333 0.708333 0.708333\n           LR_Lasso_C1   0.992647  0.600000   0.999512  0.545455   0.708333 0.708333 0.708333\n              NN_Small   1.000000  0.600000   1.000000  0.649621   0.708333 0.708333 0.708333\n        XGBoost_D3_N50   1.000000  0.600000   1.000000  0.655303   0.692308 0.750000 0.720000\n     Decision_Tree_D10   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n         LR_No_Penalty   1.000000  0.571429   1.000000  0.587121   0.714286 0.625000 0.666667\n       XGBoost_D5_N100   1.000000  0.571429   1.000000  0.571970   0.666667 0.750000 0.705882\n           LR_Ridge_C1   1.000000  0.571429   1.000000  0.564394   0.695652 0.666667 0.680851\n        SGD_Classifier   0.985294  0.571429   0.988889  0.492424   0.680000 0.708333 0.693878\n          LR_Ridge_C10   1.000000  0.571429   1.000000  0.560606   0.695652 0.666667 0.680851\n         LR_Ridge_C0.1   0.992647  0.571429   0.999756  0.575758   0.680000 0.708333 0.693878\n            SVM_Linear   1.000000  0.542857   0.000000  0.443182   0.666667 0.666667 0.666667\n              NN_Large   1.000000  0.542857   1.000000  0.549242   0.653846 0.708333 0.680000\n               NN_Deep   1.000000  0.542857   1.000000  0.562500   0.666667 0.666667 0.666667\n               NN_Adam   0.794118  0.542857   0.858364  0.496212   0.681818 0.625000 0.652174\n                   QDA   1.000000  0.514286   1.000000  0.498106   0.684211 0.541667 0.604651\n      Ridge_Classifier   1.000000  0.514286   1.000000  0.549242   0.684211 0.541667 0.604651\n           SVM_RBF_C10   1.000000  0.485714   0.000000  0.405303   0.625000 0.625000 0.625000\n           Naive_Bayes   0.580882  0.428571   0.861783  0.507576   0.750000 0.250000 0.375000\n","output_type":"stream"}],"execution_count":8},{"id":"00ff119f-316f-4b1a-88ee-1c650557a36e","cell_type":"code","source":"# ===== MODEL EVALUATION WITH RFECV =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING MODELS WITH RFECV FEATURE SELECTION\")\nprint(\"=\"*80)\n\n# Define the cross-validation strategy for RFECV\ncv = StratifiedKFold(5)\nresults_rfecv = []\n\n# List of models that are not compatible with RFE because they lack `coef_` or `feature_importances_`\nincompatible_models = {'QDA', 'Naive_Bayes', 'SVM_RBF_C1', 'SVM_RBF_C10', 'SVM_Poly_D2', 'SVM_Poly_D3', \n                       'KNN_K3', 'KNN_K5', 'KNN_K7', 'KNN_K10', 'NN_Small', 'NN_Medium', 'NN_Large', \n                       'NN_Deep', 'NN_Adam'}\n\nfor name, model in models.items():\n    if name in incompatible_models:\n        print(f\"Skipping {name}: Not compatible with RFECV.\")\n        continue\n\n    print(f\"Running RFECV for {name}...\")\n    \n    # Create a clone of the model to avoid modifying the original\n    estimator = clone(model)\n    \n    # RFECV requires a fresh estimator, so we use the cloned one\n    selector = RFECV(\n        estimator=estimator, \n        step=1, \n        cv=cv, \n        scoring='accuracy', # Optimize for accuracy\n        n_jobs=-1,\n        min_features_to_select=5 # Set a minimum number of features\n    )\n    \n    # Fit RFECV on the training data\n    selector.fit(X_train_scaled, y_train)\n    \n    print(f\"  Selected {selector.n_features_} features out of {X_train_scaled.shape[1]}\")\n    \n    # Transform the datasets to include only the selected features\n    X_train_selected = selector.transform(X_train_scaled)\n    X_test_selected = selector.transform(X_test_scaled)\n    \n    # Train a new model instance on the selected features and evaluate it\n    final_model = clone(model)\n    metrics = evaluate_model(final_model, X_train_selected, X_test_selected, y_train, y_test)\n    metrics['model'] = name\n    metrics['features_selected'] = selector.n_features_\n    results_rfecv.append(metrics)\n    \n    print(f\"  Test Accuracy with RFECV: {metrics['test_acc']:.4f}, Test AUC: {metrics['test_auc']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T04:00:59.394610Z","iopub.execute_input":"2025-11-04T04:00:59.394959Z","execution_failed":"2025-11-04T05:56:02.248Z"}},"outputs":[{"name":"stdout","text":"\n================================================================================\nTRAINING MODELS WITH RFECV FEATURE SELECTION\n================================================================================\nRunning RFECV for LR_No_Penalty...\n  Selected 12 features out of 1203\n  Test Accuracy with RFECV: 0.6000, Test AUC: 0.4659\nRunning RFECV for LR_Ridge_C1...\n  Selected 15 features out of 1203\n  Test Accuracy with RFECV: 0.5714, Test AUC: 0.5379\nRunning RFECV for LR_Ridge_C0.1...\n  Selected 5 features out of 1203\n  Test Accuracy with RFECV: 0.6571, Test AUC: 0.5568\nRunning RFECV for LR_Ridge_C10...\n  Selected 14 features out of 1203\n  Test Accuracy with RFECV: 0.4857, Test AUC: 0.4394\nRunning RFECV for LR_Lasso_C1...\n","output_type":"stream"}],"execution_count":null},{"id":"e69179bf-3d20-4758-ab8f-44d0323b6b5b","cell_type":"code","source":"# Create a DataFrame for RFECV results\nresults_rfecv_df = pd.DataFrame(results_rfecv)\nresults_rfecv_df = results_rfecv_df[['model', 'features_selected', 'train_acc', 'test_acc', 'train_auc', 'test_auc', 'precision', 'recall', 'f1']]\nresults_rfecv_df = results_rfecv_df.sort_values('test_acc', ascending=False).reset_index(drop=True)\n\nprint(\"\\n=== MODEL COMPARISON (WITH RFECV FEATURE SELECTION) ===\")\nprint(results_rfecv_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"aa93899f-d2dd-4e73-b58b-22bb0b55fb87","cell_type":"code","source":"# ===== SIDE-BY-SIDE COMPARISON: ALL FEATURES vs. RFECV =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"COMPARISON: ALL FEATURES vs. RFECV\")\nprint(\"=\"*80)\n\n# Merge the original and RFECV results for comparison\ncomparison_df = pd.merge(\n    results_df[['model', 'test_acc', 'test_auc', 'f1']],\n    results_rfecv_df[['model', 'features_selected', 'test_acc', 'test_auc', 'f1']],\n    on='model',\n    suffixes=('_All_Features', '_RFECV')\n)\n\n# Calculate the change in performance\ncomparison_df['acc_change'] = comparison_df['test_acc_RFECV'] - comparison_df['test_acc_All_Features']\ncomparison_df['num_features'] = X_train.shape[1]\n\n# Reorder columns for clarity\ncomparison_df = comparison_df[[\n    'model', \n    'num_features', \n    'features_selected', \n    'test_acc_All_Features', \n    'test_acc_RFECV', \n    'acc_change',\n    'test_auc_All_Features',\n    'test_auc_RFECV',\n    'f1_All_Features',\n    'f1_RFECV'\n]].sort_values('test_acc_RFECV', ascending=False)\n\nprint(comparison_df.to_string(index=False))","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"0c76df29-3bc3-40ff-8239-ebfe4f73e6ea","cell_type":"code","source":"# ===== FEATURE IMPORTANCE ANALYSIS =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURE IMPORTANCE COMPARISON WITH ORIGINAL STUDY\")\nprint(\"=\"*80)\n\n# Original study's important features\noriginal_features = ['MDEC-23', 'MATS2v', 'ATSC8s', 'VE3_Dt', 'CrippenMR', 'SpMax7_Bhe',\n                     'SpMin1_Bhs', 'C1SP2', 'GATS8e', 'GATS8s', 'SpMax5_Bhv', 'VE3_Dzi', 'VPC-4']\n\nfeature_names = X.columns.tolist()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"3e7adcb7-6c5e-4ea1-a603-b985281ce327","cell_type":"code","source":"def get_feature_importance(model, model_name, X_train, X_test, y_train, y_test):\n    \"\"\"Extract feature importance for different model types\"\"\"\n    # Tree-based models: use built-in feature_importances_\n    if hasattr(model, 'feature_importances_'):\n        importances = model.feature_importances_\n        method = \"Built-in (Impurity-based)\"\n    # Linear models: use absolute coefficient values\n    elif hasattr(model, 'coef_'):\n        importances = np.abs(model.coef_[0])\n        method = \"Coefficients\"\n    # Other models: use permutation importance\n    else:\n        perm_importance = permutation_importance(\n            model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n        )\n        importances = perm_importance.importances_mean\n        method = \"Permutation\"\n    \n    # Create DataFrame with feature importance\n    importance_df = pd.DataFrame({\n        'feature': feature_names,\n        'importance': importances\n    }).sort_values('importance', ascending=False)\n    \n    return importance_df, method\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"0b6cce8d-e5ec-49cf-be29-7a376fb75caf","cell_type":"code","source":"# Extract feature importance for each trained model\nprint(f\"\\nOriginal study identified {len(original_features)} important features using DTC:\")\nprint(original_features)\nprint(\"\\n\" + \"-\"*80)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"018cc492-dfa8-427f-b4d9-0a0af32ac086","cell_type":"code","source":"feature_comparison = {}\n\n\nfor name, model in models.items():\n    print(f\"\\n### {name} ###\")\n    \n    # Get feature importance\n    importance_df, method = get_feature_importance(model, name, X_train_scaled, X_test_scaled, y_train, y_test)\n    \n    # Get top 13 features (same number as original study)\n    top_13 = importance_df.head(13)\n    top_13_features = top_13['feature'].tolist()\n    \n    # Calculate overlap with original study\n    overlap = set(top_13_features) & set(original_features)\n    overlap_count = len(overlap)\n    overlap_pct = (overlap_count / len(original_features)) * 100\n    \n    print(f\"Method: {method}\")\n    print(f\"\\nTop 13 Features:\")\n    print(top_13.to_string(index=False))\n    print(f\"\\nOverlap with original study: {overlap_count}/{len(original_features)} ({overlap_pct:.1f}%)\")\n    if overlap:\n        print(f\"Matching features: {sorted(overlap)}\")\n    \n    feature_comparison[name] = {\n        'top_13': top_13_features,\n        'overlap_count': overlap_count,\n        'overlap_features': sorted(overlap),\n        'method': method\n    }\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"141f02f1-0953-440e-a338-d1cfd3ced58c","cell_type":"code","source":"# Summary comparison table\nprint(\"\\n\" + \"=\"*80)\nprint(\"SUMMARY: OVERLAP WITH ORIGINAL STUDY\")\nprint(\"=\"*80)\nsummary_df = pd.DataFrame({\n    'Model': list(feature_comparison.keys()),\n    'Overlap Count': [v['overlap_count'] for v in feature_comparison.values()],\n    'Overlap %': [(v['overlap_count']/13)*100 for v in feature_comparison.values()],\n    'Method': [v['method'] for v in feature_comparison.values()]\n}).sort_values('Overlap Count', ascending=False)\n\n\nsummary_df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.250Z"}},"outputs":[],"execution_count":null},{"id":"aaa8061a-88eb-4883-a323-3947d2edc1c2","cell_type":"code","source":"# Find features commonly selected across multiple models\nprint(\"\\n\" + \"=\"*80)\nprint(\"FEATURES SELECTED BY MULTIPLE MODELS (in top 13)\")\nprint(\"=\"*80)\n\n\nall_top_features = []\nfor comp in feature_comparison.values():\n    all_top_features.extend(comp['top_13'])\n\n\nfeature_counts = pd.Series(all_top_features).value_counts()\nfrequent_features = feature_counts[feature_counts >= 3]\n\n\nif len(frequent_features) > 0:\n    print(f\"\\nFeatures selected by 3+ models:\")\n    for feat, count in frequent_features.items():\n        in_original = \"✓\" if feat in original_features else \" \"\n        print(f\"  [{in_original}] {feat}: {count}/{len(models)} models\")\nelse:\n    print(\"No features were consistently selected across 3+ models\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"43cb9843-b0c3-430b-a2bb-320f332160da","cell_type":"code","source":"# Save detailed comparison\ncomparison_results = []\nfor model_name, comp in feature_comparison.items():\n    for i, feat in enumerate(comp['top_13'], 1):\n        comparison_results.append({\n            'model': model_name,\n            'rank': i,\n            'feature': feat,\n            'in_original_study': feat in original_features\n        })\n\n\ncomparison_df_features = pd.DataFrame(comparison_results)\ncomparison_df_features\n# comparison_df.to_csv('feature_importance_comparison.csv', index=False)\n# print(\"\\n✓ Feature importance saved to 'feature_importance_comparison.csv'\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"9a51f80f-6347-4a51-84fe-19274e0620c3","cell_type":"code","source":"# ===== CLASS WEIGHT COMPARISON =====\nprint(\"\\n\" + \"=\"*80)\nprint(\"TRAINING MODELS WITH CLASS WEIGHTS\")\nprint(\"=\"*80)\n\n# Calculate class weights\nclass_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nweight_dict = {0: class_weights[0], 1: class_weights[1]}\nprint(f\"\\nComputed class weights: {weight_dict}\")\nprint(f\"Toxic (0): {class_weights[0]:.3f}, NonToxic (1): {class_weights[1]:.3f}\")\n\n# Calculate scale_pos_weight for XGBoost\nn_toxic = np.sum(y_train == 0)\nn_nontoxic = np.sum(y_train == 1)\nscale_pos_weight = n_toxic / n_nontoxic\nprint(f\"XGBoost scale_pos_weight: {scale_pos_weight:.3f}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"58ba5d4b-a1f6-4ad5-9db8-056366ccc92f","cell_type":"code","source":"# Define models WITH class weights\nmodels_weighted = {\n    # === LINEAR MODELS ===\n    'LR_No_Penalty': LogisticRegression(penalty=None, max_iter=5000, solver='lbfgs', class_weight='balanced'),\n    'LR_Ridge_C1': LogisticRegression(penalty='l2', C=1.0, max_iter=5000, solver='lbfgs', class_weight='balanced'),\n    'LR_Ridge_C0.1': LogisticRegression(penalty='l2', C=0.1, max_iter=5000, solver='lbfgs', class_weight='balanced'),\n    'LR_Ridge_C10': LogisticRegression(penalty='l2', C=10.0, max_iter=5000, solver='lbfgs', class_weight='balanced'),\n    'LR_Lasso_C1': LogisticRegression(penalty='l1', C=1.0, max_iter=10000, solver='saga', class_weight='balanced'),\n    'LR_Lasso_C0.1': LogisticRegression(penalty='l1', C=0.1, max_iter=10000, solver='saga', class_weight='balanced'),\n    'LR_ElasticNet_L1_0.5': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=5000, class_weight='balanced'),\n    'LR_ElasticNet_L1_0.7': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.7, C=1.0, max_iter=5000, class_weight='balanced'),\n    'Ridge_Classifier': RidgeClassifier(alpha=1.0, class_weight='balanced'),\n    'SGD_Classifier': SGDClassifier(loss='log_loss', max_iter=5000, random_state=42, class_weight='balanced'),\n    \n    # === DISCRIMINANT ANALYSIS ===\n    # LDA and QDA do not support class_weight parameter\n    'LDA': LinearDiscriminantAnalysis(),\n    'QDA': QuadraticDiscriminantAnalysis(),\n    \n    # === NAIVE BAYES ===\n    # GaussianNB does not support class_weight parameter\n    'Naive_Bayes': GaussianNB(),\n    \n    # === TREE-BASED MODELS ===\n    'Decision_Tree_D5': DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced'),\n    'Decision_Tree_D10': DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced'),\n    'Decision_Tree_D20': DecisionTreeClassifier(max_depth=20, random_state=42, class_weight='balanced'),\n    'Decision_Tree_Unpruned': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n    \n    # === ENSEMBLE MODELS - BAGGING ===\n    'Random_Forest_N50': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, class_weight='balanced'),\n    'Random_Forest_N100': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n    'Random_Forest_N200': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced'),\n    'Random_Forest_Deep': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, class_weight='balanced'),\n    'Extra_Trees_N100': ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced'),\n    \n    # === ENSEMBLE MODELS - BOOSTING ===\n    # AdaBoost does not support class_weight parameter directly\n    'AdaBoost_N50': AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME'),\n    'AdaBoost_N100': AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME'),\n    # GradientBoosting does not support class_weight parameter directly\n    'GradientBoosting_N50': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42),\n    'GradientBoosting_N100': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42),\n    # XGBoost uses scale_pos_weight instead of class_weight\n    'XGBoost_D3_N50': XGBClassifier(max_depth=3, n_estimators=50, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    'XGBoost_D3_N100': XGBClassifier(max_depth=3, n_estimators=100, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    'XGBoost_D5_N100': XGBClassifier(max_depth=5, n_estimators=100, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False),\n    \n    # === SVM VARIATIONS ===\n    'SVM_Linear': SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced'),\n    'SVM_RBF_C1': SVC(kernel='rbf', C=1.0, probability=True, random_state=42, class_weight='balanced'),\n    'SVM_RBF_C10': SVC(kernel='rbf', C=10.0, probability=True, random_state=42, class_weight='balanced'),\n    'SVM_Poly_D2': SVC(kernel='poly', degree=2, probability=True, random_state=42, class_weight='balanced'),\n    'SVM_Poly_D3': SVC(kernel='poly', degree=3, probability=True, random_state=42, class_weight='balanced'),\n    \n    # === K-NEAREST NEIGHBORS ===\n    # KNN does not support class_weight parameter\n    'KNN_K3': KNeighborsClassifier(n_neighbors=3),\n    'KNN_K5': KNeighborsClassifier(n_neighbors=5),\n    'KNN_K7': KNeighborsClassifier(n_neighbors=7),\n    'KNN_K10': KNeighborsClassifier(n_neighbors=10),\n    \n    # === NEURAL NETWORKS ===\n    # MLPClassifier does not support class_weight parameter\n    'NN_Small': MLPClassifier(hidden_layer_sizes=(25,), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Medium': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Large': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Deep': MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=1000, random_state=42, early_stopping=True, solver='lbfgs'),\n    'NN_Adam': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=2000, random_state=42, early_stopping=True, solver='adam'),\n}\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"5d7d5474-b532-4962-9c16-c8d2c5125b59","cell_type":"code","source":"# Train weighted models\nresults_weighted = []\nfor name, model in models_weighted.items():\n    print(f\"Training {name} (weighted)...\")\n    metrics = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n    metrics['model'] = name\n    results_weighted.append(metrics)\n    print(f\"  Test Accuracy: {metrics['test_acc']:.4f}, Test AUC: {metrics['test_auc']:.4f}\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"c3f78025-1ad3-47af-9349-54a99773abf5","cell_type":"code","source":"# Create comparison DataFrames\nresults_df_weighted = pd.DataFrame(results_weighted)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.251Z"}},"outputs":[],"execution_count":null},{"id":"013587d2-070e-476f-a7b5-39eaa2213f9a","cell_type":"code","source":"# Merge for side-by-side comparison\ncomparison = pd.merge(\n    results_df[['model', 'test_acc', 'precision', 'recall', 'f1']],\n    results_df_weighted[['model', 'test_acc', 'precision', 'recall', 'f1']],\n    on='model',\n    suffixes=('_original', '_weighted')\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.252Z"}},"outputs":[],"execution_count":null},{"id":"f30b8512-adf6-4ab3-9de4-977f0c01e8fb","cell_type":"code","source":"# Calculate improvements\ncomparison['acc_change'] = comparison['test_acc_weighted'] - comparison['test_acc_original']\ncomparison['recall_change'] = comparison['recall_weighted'] - comparison['recall_original']\ncomparison['f1_change'] = comparison['f1_weighted'] - comparison['f1_original']\ncomparison","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.252Z"}},"outputs":[],"execution_count":null},{"id":"9582998c-0ac3-41ad-a713-5a8554aff733","cell_type":"code","source":"# Detailed confusion matrix comparison\nprint(\"\\n\" + \"=\"*100)\nprint(\"CONFUSION MATRIX COMPARISON (Original vs Weighted)\")\nprint(\"=\"*100)\n\nfor name, model_original in models.items():\n    model_weighted = models_weighted[name]\n    \n    # Get predictions\n    y_pred_original = model_original.predict(X_test_scaled)\n    y_pred_weighted = model_weighted.predict(X_test_scaled)\n    \n    # Confusion matrices\n    cm_original = confusion_matrix(y_test, y_pred_original)\n    cm_weighted = confusion_matrix(y_test, y_pred_weighted)\n    \n    print(f\"\\n### {name} ###\")\n    print(\"\\nOriginal (No Class Weights):\")\n    print(f\"                Predicted Toxic    Predicted NonToxic\")\n    print(f\"Actual Toxic          {cm_original[0,0]:3d}                 {cm_original[0,1]:3d}\")\n    print(f\"Actual NonToxic       {cm_original[1,0]:3d}                 {cm_original[1,1]:3d}\")\n    \n    print(\"\\nWith Class Weights:\")\n    print(f\"                Predicted Toxic    Predicted NonToxic\")\n    print(f\"Actual Toxic          {cm_weighted[0,0]:3d}                 {cm_weighted[0,1]:3d}\")\n    print(f\"Actual NonToxic       {cm_weighted[1,0]:3d}                 {cm_weighted[1,1]:3d}\")\n    \n    # Calculate recall for minority class\n    recall_toxic_orig = cm_original[0,0] / (cm_original[0,0] + cm_original[0,1]) if (cm_original[0,0] + cm_original[0,1]) > 0 else 0\n    recall_toxic_weighted = cm_weighted[0,0] / (cm_weighted[0,0] + cm_weighted[0,1]) if (cm_weighted[0,0] + cm_weighted[0,1]) > 0 else 0\n    \n    print(f\"\\nRecall for Toxic class: {recall_toxic_orig:.3f} → {recall_toxic_weighted:.3f} (Δ={recall_toxic_weighted-recall_toxic_orig:+.3f})\")\n    \n    # False negatives\n    fn_orig = cm_original[0,1]\n    fn_weighted = cm_weighted[0,1]\n    print(f\"False Negatives (Toxic → NonToxic): {fn_orig} → {fn_weighted} (Δ={fn_weighted-fn_orig:+d})\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.252Z"}},"outputs":[],"execution_count":null},{"id":"a337206f-8f05-4d26-a169-f0d9d540a82d","cell_type":"code","source":"# Save all comparisons\n# comparison.to_csv('class_weight_comparison.csv', index=False)\n# results_df.to_csv('model_results_original.csv', index=False)\n# results_df_weighted.to_csv('model_results_weighted.csv', index=False)\n# print(\"\\n✓ All results saved to CSV files\")\n# print(\"  - model_results_original.csv\")\n# print(\"  - model_results_weighted.csv\")\n# print(\"  - class_weight_comparison.csv\")\n# print(\"  - feature_importance_comparison.csv\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-11-04T05:56:02.252Z"}},"outputs":[],"execution_count":null},{"id":"8b28b8fd-d752-48dd-b8e4-4fa64481a352","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f3a590b3-a7bf-48a9-a497-69b9a70bed43","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"51947737-d1b0-4686-9342-980c08d8a51a","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}