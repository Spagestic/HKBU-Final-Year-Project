{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e1f368d-ac24-44a9-ba9a-6f038b857511",
   "metadata": {},
   "source": [
    "Toxicity Dataset : https://archive.ics.uci.edu/dataset/728/toxicity-2\n",
    "\n",
    "The dataset includes 171 molecules designed for functional domains of a core clock protein, CRY1, responsible for generating circadian rhythm. 56 of the molecules are toxic and the rest are non-toxic. \n",
    "\n",
    "The data consists a complete set of 1203 molecular descriptors and needs feature selection before classification since some of the features are redundant. \n",
    "\n",
    "Introductory Paper:\n",
    "Structure-based design and classifications of small molecules regulating the circadian rhythm period\n",
    "By Seref Gul, F. Rahim, Safak Isin, Fatma Yilmaz, Nuri Ozturk, M. Turkay, I. Kavakli. 2021\n",
    "https://www.semanticscholar.org/paper/Structure-based-design-and-classifications-of-small-Gul-Rahim/5944836c47bc7d1a2b0464a9a1db94d4bc7f28ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784a0694",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:07.491353Z",
     "iopub.status.busy": "2025-11-04T04:00:07.490896Z",
     "iopub.status.idle": "2025-11-04T04:00:09.332188Z",
     "shell.execute_reply": "2025-11-04T04:00:09.331256Z",
     "shell.execute_reply.started": "2025-11-04T04:00:07.491319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import RidgeClassifier, SGDClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.base import clone\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf7cb91",
   "metadata": {},
   "source": [
    "## üöÄ Performance Optimizations Implemented\n",
    "\n",
    "This notebook has been optimized for **faster model training**. Here are the key improvements:\n",
    "\n",
    "### 1. **Parallel Processing (`n_jobs=-1`)**\n",
    "- Applied to: Random Forest, Extra Trees, SVMs, KNN, Logistic Regression, SGD\n",
    "- **Effect**: Uses all available CPU cores for training and prediction\n",
    "- **Speedup**: 2-8x faster depending on CPU cores available\n",
    "\n",
    "### 2. **Early Stopping**\n",
    "- **Gradient Boosting**: `n_iter_no_change=5, validation_fraction=0.1`\n",
    "  - Stops if no improvement on validation set for 5 iterations\n",
    "  - Prevents overfitting and unnecessary training\n",
    "  \n",
    "- **XGBoost**: `early_stopping_rounds=10`\n",
    "  - Custom evaluation function in `evaluate_model()` splits training data\n",
    "  - Stops if validation score doesn't improve for 10 rounds\n",
    "  \n",
    "- **SGD Classifier**: `early_stopping=True, n_iter_no_change=10`\n",
    "  - Monitors validation score during training\n",
    "  \n",
    "- **Neural Networks**: `early_stopping=True, validation_fraction=0.1, n_iter_no_change=10`\n",
    "  - Uses validation set to determine when to stop\n",
    "\n",
    "### 3. **Reduced Iterations**\n",
    "- Neural Networks: `max_iter` reduced from 1000 ‚Üí 500\n",
    "- SGD Classifier: `max_iter` reduced from 5000 ‚Üí 1000\n",
    "- **Effect**: Faster convergence, especially with early stopping\n",
    "\n",
    "### 4. **Optimized Algorithm Settings**\n",
    "- **XGBoost**: `tree_method='hist'` (faster histogram-based tree building)\n",
    "- **Result**: 2-3x faster tree construction\n",
    "\n",
    "### Expected Performance Improvements:\n",
    "- **Total training time**: Reduced by ~60-70%\n",
    "- **Memory usage**: Slightly reduced due to smaller iterations\n",
    "- **Model quality**: Maintained or improved (early stopping prevents overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f67d9b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.334104Z",
     "iopub.status.busy": "2025-11-04T04:00:09.333635Z",
     "iopub.status.idle": "2025-11-04T04:00:09.454561Z",
     "shell.execute_reply": "2025-11-04T04:00:09.453552Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.334070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying './data.csv'\n",
      "Dataset shape: (171, 1204)\n",
      "Class distribution:\n",
      "Class\n",
      "NonToxic    115\n",
      "Toxic        56\n",
      "Name: count, dtype: int64\n",
      "Class balance:\n",
      "Class\n",
      "NonToxic    0.672515\n",
      "Toxic       0.327485\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "try:\n",
    "    data = pd.read_csv(\"/kaggle/input/toxicity/data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Trying './data.csv'\")\n",
    "    # A common alternative path structure\n",
    "    data = pd.read_csv(\"data.csv\")\n",
    "\n",
    "\n",
    "X = data.drop('Class', axis=1) if 'Class' in data.columns else data.iloc[:, :-1]\n",
    "y = data['Class'] if 'Class' in data.columns else data.iloc[:, -1]\n",
    "y_binary = (y == 'NonToxic').astype(int)\n",
    "\n",
    "\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "print(f\"Class distribution:\\n{y.value_counts()}\")\n",
    "print(f\"Class balance:\\n{y.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a982a9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.455682Z",
     "iopub.status.busy": "2025-11-04T04:00:09.455355Z",
     "iopub.status.idle": "2025-11-04T04:00:09.472382Z",
     "shell.execute_reply": "2025-11-04T04:00:09.471277Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.455652Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Shuffle and split\n",
    "np.random.seed(42)\n",
    "shuffle_idx = np.random.permutation(len(X))\n",
    "X_shuffled, y_shuffled = X.iloc[shuffle_idx].reset_index(drop=True), y_binary.iloc[shuffle_idx].reset_index(drop=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.2, random_state=42, stratify=y_shuffled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdded5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.474735Z",
     "iopub.status.busy": "2025-11-04T04:00:09.474317Z",
     "iopub.status.idle": "2025-11-04T04:00:09.542789Z",
     "shell.execute_reply": "2025-11-04T04:00:09.541671Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.474705Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (136, 1203), Test set: (35, 1203)\n"
     ]
    }
   ],
   "source": [
    "# Standardize\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"Training set: {X_train_scaled.shape}, Test set: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ccabd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.544383Z",
     "iopub.status.busy": "2025-11-04T04:00:09.543972Z",
     "iopub.status.idle": "2025-11-04T04:00:09.559411Z",
     "shell.execute_reply": "2025-11-04T04:00:09.558473Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.544357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    # === LINEAR MODELS ===\n",
    "    'LR_No_Penalty': LogisticRegression(penalty=None, max_iter=5000, solver='lbfgs', n_jobs=-1),\n",
    "    'LR_Ridge_C1': LogisticRegression(penalty='l2', C=1.0, max_iter=5000, solver='lbfgs', n_jobs=-1),\n",
    "    'LR_Ridge_C0.1': LogisticRegression(penalty='l2', C=0.1, max_iter=5000, solver='lbfgs', n_jobs=-1),\n",
    "    'LR_Ridge_C10': LogisticRegression(penalty='l2', C=10.0, max_iter=5000, solver='lbfgs', n_jobs=-1),\n",
    "    'LR_Lasso_C1': LogisticRegression(penalty='l1', C=1.0, max_iter=10000, solver='saga', n_jobs=-1), \n",
    "    'LR_Lasso_C0.1': LogisticRegression(penalty='l1', C=0.1, max_iter=10000, solver='saga', n_jobs=-1),\n",
    "    'LR_ElasticNet_L1_0.5': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=10000, n_jobs=-1),\n",
    "    'LR_ElasticNet_L1_0.7': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.7, C=1.0, max_iter=10000, n_jobs=-1),\n",
    "    'Ridge_Classifier': RidgeClassifier(alpha=1.0),\n",
    "    'SGD_Classifier': SGDClassifier(loss='log_loss', max_iter=1000, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10),\n",
    "    \n",
    "    # === DISCRIMINANT ANALYSIS ===\n",
    "    'LDA': LinearDiscriminantAnalysis(),\n",
    "    'QDA': QuadraticDiscriminantAnalysis(),\n",
    "    \n",
    "    # === NAIVE BAYES ===\n",
    "    'Naive_Bayes': GaussianNB(),\n",
    "    \n",
    "    # === TREE-BASED MODELS ===\n",
    "    'Decision_Tree_D5': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "    'Decision_Tree_D10': DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "    'Decision_Tree_D20': DecisionTreeClassifier(max_depth=20, random_state=42),\n",
    "    'Decision_Tree_Unpruned': DecisionTreeClassifier(random_state=42),\n",
    "    \n",
    "    # === ENSEMBLE MODELS - BAGGING (PARALLELIZED) ===\n",
    "    'Random_Forest_N50': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Random_Forest_N100': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Random_Forest_N200': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    'Random_Forest_Deep': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, n_jobs=-1),\n",
    "    'Extra_Trees_N100': ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1),\n",
    "    \n",
    "    # === ENSEMBLE MODELS - BOOSTING (WITH EARLY STOPPING) ===\n",
    "    'AdaBoost_N50': AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME'),\n",
    "    'AdaBoost_N100': AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME'),\n",
    "    'GradientBoosting_N50': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42, n_iter_no_change=5, validation_fraction=0.1),\n",
    "    'GradientBoosting_N100': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42, n_iter_no_change=5, validation_fraction=0.1),\n",
    "    'XGBoost_D3_N50': XGBClassifier(max_depth=3, n_estimators=50, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    'XGBoost_D3_N100': XGBClassifier(max_depth=3, n_estimators=100, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    'XGBoost_D5_N100': XGBClassifier(max_depth=5, n_estimators=100, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    \n",
    "    # === SVM VARIATIONS ===\n",
    "    'SVM_Linear': SVC(kernel='linear', probability=True, random_state=42),\n",
    "    'SVM_RBF_C1': SVC(kernel='rbf', C=1.0, probability=True, random_state=42),\n",
    "    'SVM_RBF_C10': SVC(kernel='rbf', C=10.0, probability=True, random_state=42),\n",
    "    'SVM_Poly_D2': SVC(kernel='poly', degree=2, probability=True, random_state=42),\n",
    "    'SVM_Poly_D3': SVC(kernel='poly', degree=3, probability=True, random_state=42),\n",
    "    \n",
    "    # === K-NEAREST NEIGHBORS ===\n",
    "    'KNN_K3': KNeighborsClassifier(n_neighbors=3, n_jobs=1),\n",
    "    'KNN_K5': KNeighborsClassifier(n_neighbors=5, n_jobs=1),\n",
    "    'KNN_K7': KNeighborsClassifier(n_neighbors=7, n_jobs=1),\n",
    "    'KNN_K10': KNeighborsClassifier(n_neighbors=10, n_jobs=1),\n",
    "    \n",
    "    # === NEURAL NETWORKS (WITH EARLY STOPPING & REDUCED ITERATIONS) ===\n",
    "    'NN_Small': MLPClassifier(hidden_layer_sizes=(25,), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Medium': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Large': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Deep': MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Adam': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='adam'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c7e3b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.560739Z",
     "iopub.status.busy": "2025-11-04T04:00:09.560456Z",
     "iopub.status.idle": "2025-11-04T04:00:09.583602Z",
     "shell.execute_reply": "2025-11-04T04:00:09.582534Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.560717Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Evaluation function with optimizations\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Evaluate a classification model and return metrics\"\"\"\n",
    "    X_train_selected, X_test_selected = X_train, X_test\n",
    "    \n",
    "    # ‚ö° For XGBoost models: Add early stopping callback for faster training\n",
    "    if isinstance(model, XGBClassifier):\n",
    "        # Set early stopping rounds\n",
    "        model.set_params(early_stopping_rounds=10)\n",
    "        # Create eval set for early stopping (use 20% of training data)\n",
    "        eval_size = max(int(len(X_train) * 0.2), 10)\n",
    "        X_train_fit = X_train_selected[:-eval_size]\n",
    "        X_eval = X_train_selected[-eval_size:]\n",
    "        y_train_fit = y_train[:-eval_size]\n",
    "        y_eval = y_train[-eval_size:]\n",
    "        \n",
    "        # Train with early stopping (stops if no improvement for 10 rounds)\n",
    "        model.fit(\n",
    "            X_train_fit, y_train_fit,\n",
    "            eval_set=[(X_eval, y_eval)],\n",
    "            verbose=False\n",
    "        )\n",
    "    else:\n",
    "        # For all other models, standard fit\n",
    "        model.fit(X_train_selected, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_train_pred = model.predict(X_train_selected)\n",
    "    y_test_pred = model.predict(X_test_selected)\n",
    "    \n",
    "    # Probabilities\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_train_proba = model.predict_proba(X_train_selected)[:, 1]\n",
    "        y_test_proba = model.predict_proba(X_test_selected)[:, 1]\n",
    "    else:\n",
    "        y_train_proba = model.decision_function(X_train_selected)\n",
    "        y_test_proba = model.decision_function(X_test_selected)\n",
    "    \n",
    "    return {\n",
    "        'train_acc': accuracy_score(y_train, y_train_pred),\n",
    "        'test_acc': accuracy_score(y_test, y_test_pred),\n",
    "        'train_auc': roc_auc_score(y_train, y_train_proba),\n",
    "        'test_auc': roc_auc_score(y_test, y_test_proba),\n",
    "        'precision': precision_score(y_test, y_test_pred),\n",
    "        'recall': recall_score(y_test, y_test_pred),\n",
    "        'f1': f1_score(y_test, y_test_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e0570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost model individually with detailed analysis\n",
    "print(\"=\"*80)\n",
    "print(\"INDIVIDUAL XGBOOST MODEL TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Train XGBoost_D3_N50\n",
    "xgb_model = XGBClassifier(\n",
    "    max_depth=3, \n",
    "    n_estimators=50, \n",
    "    random_state=42, \n",
    "    eval_metric='logloss', \n",
    "    use_label_encoder=False, \n",
    "    early_stopping_rounds=10,\n",
    "    tree_method='hist',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining XGBoost_D3_N50...\")\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = xgb_model.predict(X_train_scaled)\n",
    "y_test_pred = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_train_proba = xgb_model.predict_proba(X_train_scaled)[:, 1]\n",
    "y_test_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "train_auc = roc_auc_score(y_train, y_train_proba)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "precision = precision_score(y_test, y_test_pred)\n",
    "recall = recall_score(y_test, y_test_pred)\n",
    "f1 = f1_score(y_test, y_test_pred)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\n‚úì Model trained successfully!\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"  Test Accuracy:  {test_acc:.4f}\")\n",
    "print(f\"  Train AUC:      {train_auc:.4f}\")\n",
    "print(f\"  Test AUC:       {test_auc:.4f}\")\n",
    "print(f\"  Precision:      {precision:.4f}\")\n",
    "print(f\"  Recall:         {recall:.4f}\")\n",
    "print(f\"  F1 Score:       {f1:.4f}\")\n",
    "\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted Toxic    Predicted NonToxic\")\n",
    "print(f\"Actual Toxic          {cm[0,0]:3d}                 {cm[0,1]:3d}\")\n",
    "print(f\"Actual NonToxic       {cm[1,0]:3d}                 {cm[1,1]:3d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "104e44f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:09.585696Z",
     "iopub.status.busy": "2025-11-04T04:00:09.584666Z",
     "iopub.status.idle": "2025-11-04T04:00:59.374233Z",
     "shell.execute_reply": "2025-11-04T04:00:59.373246Z",
     "shell.execute_reply.started": "2025-11-04T04:00:09.585657Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING MODELS WITHOUT CLASS WEIGHTS\n",
      "================================================================================\n",
      "Training LR_No_Penalty... ‚úì (3.90s) Test Acc: 0.6000, AUC: 0.5833, CV Acc: 0.4706¬±0.0362\n",
      "Training LR_Ridge_C1... ‚úì (3.90s) Test Acc: 0.6000, AUC: 0.5833, CV Acc: 0.4706¬±0.0362\n",
      "Training LR_Ridge_C1... ‚úì (0.81s) Test Acc: 0.5714, AUC: 0.5682, CV Acc: 0.5146¬±0.0645\n",
      "Training LR_Ridge_C0.1... ‚úì (0.81s) Test Acc: 0.5714, AUC: 0.5682, CV Acc: 0.5146¬±0.0645\n",
      "Training LR_Ridge_C0.1... ‚úì (0.80s) Test Acc: 0.5714, AUC: 0.5758, CV Acc: 0.4849¬±0.0441\n",
      "Training LR_Ridge_C10... ‚úì (0.80s) Test Acc: 0.5714, AUC: 0.5758, CV Acc: 0.4849¬±0.0441\n",
      "Training LR_Ridge_C10... ‚úì (0.84s) Test Acc: 0.5429, AUC: 0.5530, CV Acc: 0.4931¬±0.0713\n",
      "Training LR_Lasso_C1... ‚úì (0.84s) Test Acc: 0.5429, AUC: 0.5530, CV Acc: 0.4931¬±0.0713\n",
      "Training LR_Lasso_C1... ‚úì (23.98s) Test Acc: 0.6000, AUC: 0.5455, CV Acc: 0.4995¬±0.0542\n",
      "Training LR_Lasso_C0.1... ‚úì (23.98s) Test Acc: 0.6000, AUC: 0.5455, CV Acc: 0.4995¬±0.0542\n",
      "Training LR_Lasso_C0.1... ‚úì (7.00s) Test Acc: 0.6857, AUC: 0.5644, CV Acc: 0.6690¬±0.0048\n",
      "Training LR_ElasticNet_L1_0.5... ‚úì (7.00s) Test Acc: 0.6857, AUC: 0.5644, CV Acc: 0.6690¬±0.0048\n",
      "Training LR_ElasticNet_L1_0.5... ‚úì (20.94s) Test Acc: 0.6000, AUC: 0.5417, CV Acc: 0.5140¬±0.0753\n",
      "Training LR_ElasticNet_L1_0.7... ‚úì (20.94s) Test Acc: 0.6000, AUC: 0.5417, CV Acc: 0.5140¬±0.0753\n",
      "Training LR_ElasticNet_L1_0.7... ‚úì (25.07s) Test Acc: 0.6000, AUC: 0.5455, CV Acc: 0.5291¬±0.0734\n",
      "Training Ridge_Classifier... ‚úì (0.11s) Test Acc: 0.5143, AUC: 0.5492, CV Acc: 0.5370¬±0.0406\n",
      "Training SGD_Classifier... ‚úì (25.07s) Test Acc: 0.6000, AUC: 0.5455, CV Acc: 0.5291¬±0.0734\n",
      "Training Ridge_Classifier... ‚úì (0.11s) Test Acc: 0.5143, AUC: 0.5492, CV Acc: 0.5370¬±0.0406\n",
      "Training SGD_Classifier... ‚úì (0.16s) Test Acc: 0.6000, AUC: 0.5795, CV Acc: 0.4778¬±0.0296\n",
      "Training LDA... ‚úì (0.16s) Test Acc: 0.6000, AUC: 0.5795, CV Acc: 0.4778¬±0.0296\n",
      "Training LDA... ‚úì (0.79s) Test Acc: 0.6000, AUC: 0.5473, CV Acc: 0.4638¬±0.0632\n",
      "Training QDA... ‚úì (0.79s) Test Acc: 0.6000, AUC: 0.5473, CV Acc: 0.4638¬±0.0632\n",
      "Training QDA... ‚úì (0.71s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4630¬±0.0703\n",
      "Training Naive_Bayes... ‚úì (0.19s) Test Acc: 0.4286, AUC: 0.5076, CV Acc: 0.4032¬±0.1106\n",
      "Training Decision_Tree_D5... ‚úì (0.71s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4630¬±0.0703\n",
      "Training Naive_Bayes... ‚úì (0.19s) Test Acc: 0.4286, AUC: 0.5076, CV Acc: 0.4032¬±0.1106\n",
      "Training Decision_Tree_D5... ‚úì (0.48s) Test Acc: 0.6286, AUC: 0.5795, CV Acc: 0.5082¬±0.0830\n",
      "Training Decision_Tree_D10... ‚úì (0.48s) Test Acc: 0.6286, AUC: 0.5795, CV Acc: 0.5082¬±0.0830\n",
      "Training Decision_Tree_D10... ‚úì (0.51s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Decision_Tree_D20... ‚úì (0.51s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Decision_Tree_D20... ‚úì (0.47s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Decision_Tree_Unpruned... ‚úì (0.47s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Decision_Tree_Unpruned... ‚úì (0.49s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Random_Forest_N50... ‚úì (0.49s) Test Acc: 0.6000, AUC: 0.5852, CV Acc: 0.4862¬±0.1337\n",
      "Training Random_Forest_N50... ‚úì (1.64s) Test Acc: 0.6571, AUC: 0.6383, CV Acc: 0.5437¬±0.0671\n",
      "Training Random_Forest_N100... ‚úì (1.64s) Test Acc: 0.6571, AUC: 0.6383, CV Acc: 0.5437¬±0.0671\n",
      "Training Random_Forest_N100... ‚úì (2.37s) Test Acc: 0.6286, AUC: 0.6212, CV Acc: 0.5735¬±0.0497\n",
      "Training Random_Forest_N200... ‚úì (2.37s) Test Acc: 0.6286, AUC: 0.6212, CV Acc: 0.5735¬±0.0497\n",
      "Training Random_Forest_N200... ‚úì (3.42s) Test Acc: 0.6286, AUC: 0.6136, CV Acc: 0.5587¬±0.0623\n",
      "Training Random_Forest_Deep... ‚úì (3.42s) Test Acc: 0.6286, AUC: 0.6136, CV Acc: 0.5587¬±0.0623\n",
      "Training Random_Forest_Deep... ‚úì (2.36s) Test Acc: 0.6286, AUC: 0.6307, CV Acc: 0.5735¬±0.0497\n",
      "Training Extra_Trees_N100... ‚úì (2.36s) Test Acc: 0.6286, AUC: 0.6307, CV Acc: 0.5735¬±0.0497\n",
      "Training Extra_Trees_N100... ‚úì (2.14s) Test Acc: 0.6286, AUC: 0.6629, CV Acc: 0.5508¬±0.0749\n",
      "Training AdaBoost_N50... ‚úì (2.14s) Test Acc: 0.6286, AUC: 0.6629, CV Acc: 0.5508¬±0.0749\n",
      "Training AdaBoost_N50... ‚úì (6.00s) Test Acc: 0.6286, AUC: 0.6364, CV Acc: 0.5370¬±0.1048\n",
      "Training AdaBoost_N100... ‚úì (6.00s) Test Acc: 0.6286, AUC: 0.6364, CV Acc: 0.5370¬±0.1048\n",
      "Training AdaBoost_N100... ‚úì (9.55s) Test Acc: 0.6286, AUC: 0.5985, CV Acc: 0.5442¬±0.0949\n",
      "Training GradientBoosting_N50... ‚úì (9.55s) Test Acc: 0.6286, AUC: 0.5985, CV Acc: 0.5442¬±0.0949\n",
      "Training GradientBoosting_N50... ‚úì (1.33s) Test Acc: 0.5714, AUC: 0.6117, CV Acc: 0.5810¬±0.0364\n",
      "Training GradientBoosting_N100... ‚úì (1.33s) Test Acc: 0.5714, AUC: 0.6117, CV Acc: 0.5810¬±0.0364\n",
      "Training GradientBoosting_N100... ‚úì (1.33s) Test Acc: 0.5714, AUC: 0.6117, CV Acc: 0.5810¬±0.0364\n",
      "Training XGBoost_D3_N50... ‚úì (1.33s) Test Acc: 0.5714, AUC: 0.6117, CV Acc: 0.5810¬±0.0364\n",
      "Training XGBoost_D3_N50... ‚úì (0.97s) Test Acc: 0.6857, AUC: 0.6269, CV Acc: 0.5074¬±0.0949\n",
      "Training XGBoost_D3_N100... ‚úì (0.97s) Test Acc: 0.6857, AUC: 0.6269, CV Acc: 0.5074¬±0.0949\n",
      "Training XGBoost_D3_N100... ‚úì (1.40s) Test Acc: 0.6857, AUC: 0.6269, CV Acc: 0.5143¬±0.0937\n",
      "Training XGBoost_D5_N100... ‚úì (1.40s) Test Acc: 0.6857, AUC: 0.6269, CV Acc: 0.5143¬±0.0937\n",
      "Training XGBoost_D5_N100... ‚úì (1.62s) Test Acc: 0.6857, AUC: 0.6439, CV Acc: 0.5439¬±0.0735\n",
      "Training SVM_Linear... ‚úì (1.62s) Test Acc: 0.6857, AUC: 0.6439, CV Acc: 0.5439¬±0.0735\n",
      "Training SVM_Linear... ‚úì (0.21s) Test Acc: 0.5429, AUC: 0.4432, CV Acc: 0.5151¬±0.0761\n",
      "Training SVM_RBF_C1... ‚úì (0.20s) Test Acc: 0.6857, AUC: 0.3826, CV Acc: 0.6394¬±0.0305\n",
      "Training SVM_RBF_C10... ‚úì (0.21s) Test Acc: 0.5429, AUC: 0.4432, CV Acc: 0.5151¬±0.0761\n",
      "Training SVM_RBF_C1... ‚úì (0.20s) Test Acc: 0.6857, AUC: 0.3826, CV Acc: 0.6394¬±0.0305\n",
      "Training SVM_RBF_C10... ‚úì (0.20s) Test Acc: 0.4857, AUC: 0.4053, CV Acc: 0.5000¬±0.0828\n",
      "Training SVM_Poly_D2... ‚úì (0.14s) Test Acc: 0.6857, AUC: 0.3826, CV Acc: 0.6468¬±0.0214\n",
      "Training SVM_Poly_D3... ‚úì (0.20s) Test Acc: 0.4857, AUC: 0.4053, CV Acc: 0.5000¬±0.0828\n",
      "Training SVM_Poly_D2... ‚úì (0.14s) Test Acc: 0.6857, AUC: 0.3826, CV Acc: 0.6468¬±0.0214\n",
      "Training SVM_Poly_D3... ‚úì (0.15s) Test Acc: 0.6857, AUC: 0.3333, CV Acc: 0.6468¬±0.0214\n",
      "Training KNN_K3... ‚úì (0.15s) Test Acc: 0.6857, AUC: 0.3333, CV Acc: 0.6468¬±0.0214\n",
      "Training KNN_K3... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "        \"wmic CPU Get NumberOfCores /Format:csv\".split(),\n",
      "        capture_output=True,\n",
      "        text=True,\n",
      "    )\n",
      "  File \"c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 556, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "         ~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1038, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        pass_fds, cwd, env,\n",
      "                        ^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "                        gid, gids, uid, umask,\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^\n",
      "                        start_new_session, process_group)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\visha\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py\", line 1550, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n",
      "                       ~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "                             # no special security\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<4 lines>...\n",
      "                             cwd,\n",
      "                             ^^^^\n",
      "                             startupinfo)\n",
      "                             ^^^^^^^^^^^^\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì (0.59s) Test Acc: 0.6571, AUC: 0.6951, CV Acc: 0.5302¬±0.1277\n",
      "Training KNN_K5... ‚úì (0.55s) Test Acc: 0.6571, AUC: 0.7027, CV Acc: 0.5884¬±0.1025\n",
      "Training KNN_K7... ‚úì (0.55s) Test Acc: 0.6571, AUC: 0.7027, CV Acc: 0.5884¬±0.1025\n",
      "Training KNN_K7... ‚úì (0.55s) Test Acc: 0.6571, AUC: 0.6667, CV Acc: 0.5656¬±0.1101\n",
      "Training KNN_K10... ‚úì (0.55s) Test Acc: 0.6571, AUC: 0.6667, CV Acc: 0.5656¬±0.1101\n",
      "Training KNN_K10... ‚úì (0.57s) Test Acc: 0.6571, AUC: 0.6553, CV Acc: 0.5360¬±0.0954\n",
      "Training NN_Small... ‚úì (0.57s) Test Acc: 0.6571, AUC: 0.6553, CV Acc: 0.5360¬±0.0954\n",
      "Training NN_Small... ‚úì (0.65s) Test Acc: 0.6000, AUC: 0.6496, CV Acc: 0.4632¬±0.0549\n",
      "Training NN_Medium... ‚úì (0.65s) Test Acc: 0.6000, AUC: 0.6496, CV Acc: 0.4632¬±0.0549\n",
      "Training NN_Medium... ‚úì (0.83s) Test Acc: 0.6286, AUC: 0.5360, CV Acc: 0.4929¬±0.0568\n",
      "Training NN_Large... ‚úì (0.83s) Test Acc: 0.6286, AUC: 0.5360, CV Acc: 0.4929¬±0.0568\n",
      "Training NN_Large... ‚úì (1.31s) Test Acc: 0.5429, AUC: 0.5492, CV Acc: 0.5071¬±0.0568\n",
      "Training NN_Deep... ‚úì (1.31s) Test Acc: 0.5429, AUC: 0.5492, CV Acc: 0.5071¬±0.0568\n",
      "Training NN_Deep... ‚úì (1.46s) Test Acc: 0.5429, AUC: 0.5625, CV Acc: 0.5003¬±0.0403\n",
      "Training NN_Adam... ‚úì (1.46s) Test Acc: 0.5429, AUC: 0.5625, CV Acc: 0.5003¬±0.0403\n",
      "Training NN_Adam... ‚úì (0.33s) Test Acc: 0.5429, AUC: 0.4962, CV Acc: 0.4854¬±0.0831\n",
      "\n",
      "‚è±Ô∏è  Training time summary:\n",
      "Fastest: Ridge_Classifier (0.11s)\n",
      "Slowest: LR_ElasticNet_L1_0.7 (25.07s)\n",
      "Total time: 129.12s\n",
      "‚úì (0.33s) Test Acc: 0.5429, AUC: 0.4962, CV Acc: 0.4854¬±0.0831\n",
      "\n",
      "‚è±Ô∏è  Training time summary:\n",
      "Fastest: Ridge_Classifier (0.11s)\n",
      "Slowest: LR_ElasticNet_L1_0.7 (25.07s)\n",
      "Total time: 129.12s\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate all models (WITHOUT class weights) - WITH TIMING\n",
    "# Following scikit-learn best practices from Context7\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import clone\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODELS WITHOUT CLASS WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "results = []\n",
    "model_times = {}  # Track training time per model\n",
    "\n",
    "# Define scoring metrics for comprehensive evaluation\n",
    "scoring_metrics = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"Training {name}...\", end=\" \")\n",
    "    \n",
    "    # Clone the model to ensure fresh instance (scikit-learn best practice)\n",
    "    model_clone = clone(model)\n",
    "    \n",
    "    # Use cross_validate for comprehensive metrics (Context7 recommendation)\n",
    "    cv_results = cross_validate(\n",
    "        model_clone, \n",
    "        X_train_scaled, \n",
    "        y_train, \n",
    "        cv=5,  # 5-fold cross-validation\n",
    "        scoring=scoring_metrics,\n",
    "        return_train_score=True,\n",
    "        n_jobs=1  # Prevent nested parallelization\n",
    "    )\n",
    "    \n",
    "    # Also evaluate on held-out test set\n",
    "    model_clone.fit(X_train_scaled, y_train)\n",
    "    metrics = evaluate_model(model_clone, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    metrics['model'] = name\n",
    "    \n",
    "    # Add cross-validation scores to metrics\n",
    "    metrics['cv_accuracy_mean'] = cv_results['test_accuracy'].mean()\n",
    "    metrics['cv_accuracy_std'] = cv_results['test_accuracy'].std()\n",
    "    \n",
    "    results.append(metrics)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    model_times[name] = elapsed\n",
    "    \n",
    "    print(f\"‚úì ({elapsed:.2f}s) Test Acc: {metrics['test_acc']:.4f}, AUC: {metrics['test_auc']:.4f}, CV Acc: {metrics['cv_accuracy_mean']:.4f}¬±{metrics['cv_accuracy_std']:.4f}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Training time summary:\")\n",
    "fastest = min(model_times, key=model_times.get)\n",
    "slowest = max(model_times, key=model_times.get)\n",
    "print(f\"Fastest: {fastest} ({model_times[fastest]:.2f}s)\")\n",
    "print(f\"Slowest: {slowest} ({model_times[slowest]:.2f}s)\")\n",
    "print(f\"Total time: {sum(model_times.values()):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef92fa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-04T04:00:59.375588Z",
     "iopub.status.busy": "2025-11-04T04:00:59.375223Z",
     "iopub.status.idle": "2025-11-04T04:00:59.393682Z",
     "shell.execute_reply": "2025-11-04T04:00:59.392885Z",
     "shell.execute_reply.started": "2025-11-04T04:00:59.375555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MODEL COMPARISON (NO CLASS WEIGHTS) ===\n",
      "                 model  train_acc  test_acc  train_auc  test_auc  precision   recall       f1\n",
      "           SVM_Poly_D2   0.720588  0.685714   0.011966  0.382576   0.685714 1.000000 0.813559\n",
      "            SVM_RBF_C1   0.727941  0.685714   0.010501  0.382576   0.685714 1.000000 0.813559\n",
      "         LR_Lasso_C0.1   0.669118  0.685714   0.723077  0.564394   0.685714 1.000000 0.813559\n",
      "           SVM_Poly_D3   0.720588  0.685714   0.006105  0.333333   0.685714 1.000000 0.813559\n",
      "     Random_Forest_N50   1.000000  0.657143   1.000000  0.638258   0.714286 0.833333 0.769231\n",
      "                KNN_K3   0.757353  0.657143   0.779487  0.695076   0.750000 0.750000 0.750000\n",
      "                KNN_K5   0.713235  0.657143   0.757021  0.702652   0.714286 0.833333 0.769231\n",
      "                KNN_K7   0.727941  0.657143   0.728816  0.666667   0.714286 0.833333 0.769231\n",
      "               KNN_K10   0.654412  0.657143   0.682051  0.655303   0.730769 0.791667 0.760000\n",
      "  GradientBoosting_N50   1.000000  0.657143   1.000000  0.685606   0.730769 0.791667 0.760000\n",
      " GradientBoosting_N100   1.000000  0.657143   1.000000  0.625000   0.730769 0.791667 0.760000\n",
      "       XGBoost_D3_N100   1.000000  0.657143   1.000000  0.666667   0.714286 0.833333 0.769231\n",
      "          AdaBoost_N50   1.000000  0.628571   1.000000  0.636364   0.720000 0.750000 0.734694\n",
      "         AdaBoost_N100   1.000000  0.628571   1.000000  0.598485   0.720000 0.750000 0.734694\n",
      "    Random_Forest_Deep   1.000000  0.628571   1.000000  0.630682   0.703704 0.791667 0.745098\n",
      "    Random_Forest_N200   1.000000  0.628571   1.000000  0.613636   0.703704 0.791667 0.745098\n",
      "    Random_Forest_N100   1.000000  0.628571   1.000000  0.621212   0.703704 0.791667 0.745098\n",
      "      Extra_Trees_N100   1.000000  0.628571   1.000000  0.662879   0.703704 0.791667 0.745098\n",
      "      Decision_Tree_D5   0.948529  0.628571   0.989866  0.579545   0.761905 0.666667 0.711111\n",
      "             NN_Medium   1.000000  0.628571   1.000000  0.535985   0.739130 0.708333 0.723404\n",
      "Decision_Tree_Unpruned   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n",
      "     Decision_Tree_D20   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n",
      "                   LDA   0.955882  0.600000   0.993407  0.547348   0.750000 0.625000 0.681818\n",
      "  LR_ElasticNet_L1_0.7   0.992647  0.600000   1.000000  0.545455   0.708333 0.708333 0.708333\n",
      "  LR_ElasticNet_L1_0.5   1.000000  0.600000   1.000000  0.541667   0.708333 0.708333 0.708333\n",
      "           LR_Lasso_C1   0.992647  0.600000   0.999512  0.545455   0.708333 0.708333 0.708333\n",
      "              NN_Small   1.000000  0.600000   1.000000  0.649621   0.708333 0.708333 0.708333\n",
      "        XGBoost_D3_N50   1.000000  0.600000   1.000000  0.655303   0.692308 0.750000 0.720000\n",
      "     Decision_Tree_D10   1.000000  0.600000   1.000000  0.585227   0.750000 0.625000 0.681818\n",
      "         LR_No_Penalty   1.000000  0.571429   1.000000  0.587121   0.714286 0.625000 0.666667\n",
      "       XGBoost_D5_N100   1.000000  0.571429   1.000000  0.571970   0.666667 0.750000 0.705882\n",
      "           LR_Ridge_C1   1.000000  0.571429   1.000000  0.564394   0.695652 0.666667 0.680851\n",
      "        SGD_Classifier   0.985294  0.571429   0.988889  0.492424   0.680000 0.708333 0.693878\n",
      "          LR_Ridge_C10   1.000000  0.571429   1.000000  0.560606   0.695652 0.666667 0.680851\n",
      "         LR_Ridge_C0.1   0.992647  0.571429   0.999756  0.575758   0.680000 0.708333 0.693878\n",
      "            SVM_Linear   1.000000  0.542857   0.000000  0.443182   0.666667 0.666667 0.666667\n",
      "              NN_Large   1.000000  0.542857   1.000000  0.549242   0.653846 0.708333 0.680000\n",
      "               NN_Deep   1.000000  0.542857   1.000000  0.562500   0.666667 0.666667 0.666667\n",
      "               NN_Adam   0.794118  0.542857   0.858364  0.496212   0.681818 0.625000 0.652174\n",
      "                   QDA   1.000000  0.514286   1.000000  0.498106   0.684211 0.541667 0.604651\n",
      "      Ridge_Classifier   1.000000  0.514286   1.000000  0.549242   0.684211 0.541667 0.604651\n",
      "           SVM_RBF_C10   1.000000  0.485714   0.000000  0.405303   0.625000 0.625000 0.625000\n",
      "           Naive_Bayes   0.580882  0.428571   0.861783  0.507576   0.750000 0.250000 0.375000\n"
     ]
    }
   ],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df[['model', 'train_acc', 'test_acc', 'train_auc', 'test_auc', 'precision', 'recall', 'f1']]\n",
    "results_df = results_df.sort_values('test_acc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON (NO CLASS WEIGHTS) ===\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ff119f-316f-4b1a-88ee-1c650557a36e",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.248Z",
     "iopub.execute_input": "2025-11-04T04:00:59.394959Z",
     "iopub.status.busy": "2025-11-04T04:00:59.394610Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TRAINING MODELS WITH RFECV FEATURE SELECTION\n",
      "================================================================================\n",
      "Running RFECV for LR_No_Penalty...\n",
      "  Selected 12 features out of 1203\n",
      "  Test Accuracy with RFECV: 0.6000, Test AUC: 0.4659\n",
      "Running RFECV for LR_Ridge_C1...\n",
      "  Selected 15 features out of 1203\n",
      "  Test Accuracy with RFECV: 0.5714, Test AUC: 0.5379\n",
      "Running RFECV for LR_Ridge_C0.1...\n",
      "  Selected 5 features out of 1203\n",
      "  Test Accuracy with RFECV: 0.6571, Test AUC: 0.5568\n",
      "Running RFECV for LR_Ridge_C10...\n",
      "  Selected 14 features out of 1203\n",
      "  Test Accuracy with RFECV: 0.4857, Test AUC: 0.4394\n",
      "Running RFECV for LR_Lasso_C1...\n"
     ]
    }
   ],
   "source": [
    "# ===== MODEL EVALUATION WITH RFECV =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODELS WITH RFECV FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define the cross-validation strategy for RFECV\n",
    "cv = StratifiedKFold(5)\n",
    "results_rfecv = []\n",
    "\n",
    "# List of models that are not compatible with RFE because they lack `coef_` or `feature_importances_`\n",
    "incompatible_models = {'QDA', 'Naive_Bayes', 'SVM_RBF_C1', 'SVM_RBF_C10', 'SVM_Poly_D2', 'SVM_Poly_D3', \n",
    "                       'KNN_K3', 'KNN_K5', 'KNN_K7', 'KNN_K10', 'NN_Small', 'NN_Medium', 'NN_Large', \n",
    "                       'NN_Deep', 'NN_Adam'}\n",
    "\n",
    "for name, model in models.items():\n",
    "    if name in incompatible_models:\n",
    "        print(f\"Skipping {name}: Not compatible with RFECV.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Running RFECV for {name}...\")\n",
    "    \n",
    "    # Create a clone of the model to avoid modifying the original\n",
    "    estimator = clone(model)\n",
    "    \n",
    "    # RFECV requires a fresh estimator, so we use the cloned one\n",
    "    selector = RFECV(\n",
    "        estimator=estimator, \n",
    "        step=1, \n",
    "        cv=cv, \n",
    "        scoring='accuracy', # Optimize for accuracy\n",
    "        n_jobs=-1,\n",
    "        min_features_to_select=5 # Set a minimum number of features\n",
    "    )\n",
    "    \n",
    "    # Fit RFECV on the training data\n",
    "    selector.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    print(f\"  Selected {selector.n_features_} features out of {X_train_scaled.shape[1]}\")\n",
    "    \n",
    "    # Transform the datasets to include only the selected features\n",
    "    X_train_selected = selector.transform(X_train_scaled)\n",
    "    X_test_selected = selector.transform(X_test_scaled)\n",
    "    \n",
    "    # Train a new model instance on the selected features and evaluate it\n",
    "    final_model = clone(model)\n",
    "    metrics = evaluate_model(final_model, X_train_selected, X_test_selected, y_train, y_test)\n",
    "    metrics['model'] = name\n",
    "    metrics['features_selected'] = selector.n_features_\n",
    "    results_rfecv.append(metrics)\n",
    "    \n",
    "    print(f\"  Test Accuracy with RFECV: {metrics['test_acc']:.4f}, Test AUC: {metrics['test_auc']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69179bf-3d20-4758-ab8f-44d0323b6b5b",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame for RFECV results\n",
    "results_rfecv_df = pd.DataFrame(results_rfecv)\n",
    "results_rfecv_df = results_rfecv_df[['model', 'features_selected', 'train_acc', 'test_acc', 'train_auc', 'test_auc', 'precision', 'recall', 'f1']]\n",
    "results_rfecv_df = results_rfecv_df.sort_values('test_acc', ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(\"\\n=== MODEL COMPARISON (WITH RFECV FEATURE SELECTION) ===\")\n",
    "print(results_rfecv_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa93899f-d2dd-4e73-b58b-22bb0b55fb87",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== SIDE-BY-SIDE COMPARISON: ALL FEATURES vs. RFECV =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON: ALL FEATURES vs. RFECV\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Merge the original and RFECV results for comparison\n",
    "comparison_df = pd.merge(\n",
    "    results_df[['model', 'test_acc', 'test_auc', 'f1']],\n",
    "    results_rfecv_df[['model', 'features_selected', 'test_acc', 'test_auc', 'f1']],\n",
    "    on='model',\n",
    "    suffixes=('_All_Features', '_RFECV')\n",
    ")\n",
    "\n",
    "# Calculate the change in performance\n",
    "comparison_df['acc_change'] = comparison_df['test_acc_RFECV'] - comparison_df['test_acc_All_Features']\n",
    "comparison_df['num_features'] = X_train.shape[1]\n",
    "\n",
    "# Reorder columns for clarity\n",
    "comparison_df = comparison_df[[\n",
    "    'model', \n",
    "    'num_features', \n",
    "    'features_selected', \n",
    "    'test_acc_All_Features', \n",
    "    'test_acc_RFECV', \n",
    "    'acc_change',\n",
    "    'test_auc_All_Features',\n",
    "    'test_auc_RFECV',\n",
    "    'f1_All_Features',\n",
    "    'f1_RFECV'\n",
    "]].sort_values('test_acc_RFECV', ascending=False)\n",
    "\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c76df29-3bc3-40ff-8239-ebfe4f73e6ea",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== FEATURE IMPORTANCE ANALYSIS =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE COMPARISON WITH ORIGINAL STUDY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Original study's important features\n",
    "original_features = ['MDEC-23', 'MATS2v', 'ATSC8s', 'VE3_Dt', 'CrippenMR', 'SpMax7_Bhe',\n",
    "                     'SpMin1_Bhs', 'C1SP2', 'GATS8e', 'GATS8s', 'SpMax5_Bhv', 'VE3_Dzi', 'VPC-4']\n",
    "\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7adcb7-6c5e-4ea1-a603-b985281ce327",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_feature_importance(model, model_name, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Extract feature importance for different model types\"\"\"\n",
    "    # Tree-based models: use built-in feature_importances_\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        method = \"Built-in (Impurity-based)\"\n",
    "    # Linear models: use absolute coefficient values\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        importances = np.abs(model.coef_[0])\n",
    "        method = \"Coefficients\"\n",
    "    # Other models: use permutation importance\n",
    "    else:\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        importances = perm_importance.importances_mean\n",
    "        method = \"Permutation\"\n",
    "    \n",
    "    # Create DataFrame with feature importance\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    return importance_df, method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6cce8d-e5ec-49cf-be29-7a376fb75caf",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Extract feature importance for each trained model\n",
    "print(f\"\\nOriginal study identified {len(original_features)} important features using DTC:\")\n",
    "print(original_features)\n",
    "print(\"\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018cc492-dfa8-427f-b4d9-0a0af32ac086",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "feature_comparison = {}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n### {name} ###\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    importance_df, method = get_feature_importance(model, name, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    \n",
    "    # Get top 13 features (same number as original study)\n",
    "    top_13 = importance_df.head(13)\n",
    "    top_13_features = top_13['feature'].tolist()\n",
    "    \n",
    "    # Calculate overlap with original study\n",
    "    overlap = set(top_13_features) & set(original_features)\n",
    "    overlap_count = len(overlap)\n",
    "    overlap_pct = (overlap_count / len(original_features)) * 100\n",
    "    \n",
    "    print(f\"Method: {method}\")\n",
    "    print(f\"\\nTop 13 Features:\")\n",
    "    print(top_13.to_string(index=False))\n",
    "    print(f\"\\nOverlap with original study: {overlap_count}/{len(original_features)} ({overlap_pct:.1f}%)\")\n",
    "    if overlap:\n",
    "        print(f\"Matching features: {sorted(overlap)}\")\n",
    "    \n",
    "    feature_comparison[name] = {\n",
    "        'top_13': top_13_features,\n",
    "        'overlap_count': overlap_count,\n",
    "        'overlap_features': sorted(overlap),\n",
    "        'method': method\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141f02f1-0953-440e-a338-d1cfd3ced58c",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.250Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Summary comparison table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: OVERLAP WITH ORIGINAL STUDY\")\n",
    "print(\"=\"*80)\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': list(feature_comparison.keys()),\n",
    "    'Overlap Count': [v['overlap_count'] for v in feature_comparison.values()],\n",
    "    'Overlap %': [(v['overlap_count']/13)*100 for v in feature_comparison.values()],\n",
    "    'Method': [v['method'] for v in feature_comparison.values()]\n",
    "}).sort_values('Overlap Count', ascending=False)\n",
    "\n",
    "\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa8061a-88eb-4883-a323-3947d2edc1c2",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Find features commonly selected across multiple models\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FEATURES SELECTED BY MULTIPLE MODELS (in top 13)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "all_top_features = []\n",
    "for comp in feature_comparison.values():\n",
    "    all_top_features.extend(comp['top_13'])\n",
    "\n",
    "\n",
    "feature_counts = pd.Series(all_top_features).value_counts()\n",
    "frequent_features = feature_counts[feature_counts >= 3]\n",
    "\n",
    "\n",
    "if len(frequent_features) > 0:\n",
    "    print(f\"\\nFeatures selected by 3+ models:\")\n",
    "    for feat, count in frequent_features.items():\n",
    "        in_original = \"‚úì\" if feat in original_features else \" \"\n",
    "        print(f\"  [{in_original}] {feat}: {count}/{len(models)} models\")\n",
    "else:\n",
    "    print(\"No features were consistently selected across 3+ models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb9843-b0c3-430b-a2bb-320f332160da",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save detailed comparison\n",
    "comparison_results = []\n",
    "for model_name, comp in feature_comparison.items():\n",
    "    for i, feat in enumerate(comp['top_13'], 1):\n",
    "        comparison_results.append({\n",
    "            'model': model_name,\n",
    "            'rank': i,\n",
    "            'feature': feat,\n",
    "            'in_original_study': feat in original_features\n",
    "        })\n",
    "\n",
    "\n",
    "comparison_df_features = pd.DataFrame(comparison_results)\n",
    "comparison_df_features\n",
    "# comparison_df.to_csv('feature_importance_comparison.csv', index=False)\n",
    "# print(\"\\n‚úì Feature importance saved to 'feature_importance_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51f80f-6347-4a51-84fe-19274e0620c3",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ===== CLASS WEIGHT COMPARISON =====\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING MODELS WITH CLASS WEIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "print(f\"\\nComputed class weights: {weight_dict}\")\n",
    "print(f\"Toxic (0): {class_weights[0]:.3f}, NonToxic (1): {class_weights[1]:.3f}\")\n",
    "\n",
    "# Calculate scale_pos_weight for XGBoost\n",
    "n_toxic = np.sum(y_train == 0)\n",
    "n_nontoxic = np.sum(y_train == 1)\n",
    "scale_pos_weight = n_toxic / n_nontoxic\n",
    "print(f\"XGBoost scale_pos_weight: {scale_pos_weight:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba5d4b-a1f6-4ad5-9db8-056366ccc92f",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define models WITH class weights - OPTIMIZED\n",
    "models_weighted = {\n",
    "    # === LINEAR MODELS ===\n",
    "    'LR_No_Penalty': LogisticRegression(penalty=None, max_iter=5000, solver='lbfgs', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_Ridge_C1': LogisticRegression(penalty='l2', C=1.0, max_iter=5000, solver='lbfgs', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_Ridge_C0.1': LogisticRegression(penalty='l2', C=0.1, max_iter=5000, solver='lbfgs', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_Ridge_C10': LogisticRegression(penalty='l2', C=10.0, max_iter=5000, solver='lbfgs', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_Lasso_C1': LogisticRegression(penalty='l1', C=1.0, max_iter=10000, solver='saga', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_Lasso_C0.1': LogisticRegression(penalty='l1', C=0.1, max_iter=10000, solver='saga', class_weight='balanced', n_jobs=-1),\n",
    "    'LR_ElasticNet_L1_0.5': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, C=1.0, max_iter=10000, class_weight='balanced', n_jobs=-1),\n",
    "    'LR_ElasticNet_L1_0.7': LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.7, C=1.0, max_iter=10000, class_weight='balanced', n_jobs=-1),\n",
    "    'Ridge_Classifier': RidgeClassifier(alpha=1.0, class_weight='balanced'),\n",
    "    'SGD_Classifier': SGDClassifier(loss='log_loss', max_iter=1000, random_state=42, class_weight='balanced', early_stopping=True, validation_fraction=0.1, n_iter_no_change=10),\n",
    "    \n",
    "    # === DISCRIMINANT ANALYSIS ===\n",
    "    'LDA': LinearDiscriminantAnalysis(),\n",
    "    'QDA': QuadraticDiscriminantAnalysis(),\n",
    "    \n",
    "    # === NAIVE BAYES ===\n",
    "    'Naive_Bayes': GaussianNB(),\n",
    "    \n",
    "    # === TREE-BASED MODELS ===\n",
    "    'Decision_Tree_D5': DecisionTreeClassifier(max_depth=5, random_state=42, class_weight='balanced'),\n",
    "    'Decision_Tree_D10': DecisionTreeClassifier(max_depth=10, random_state=42, class_weight='balanced'),\n",
    "    'Decision_Tree_D20': DecisionTreeClassifier(max_depth=20, random_state=42, class_weight='balanced'),\n",
    "    'Decision_Tree_Unpruned': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "    \n",
    "    # === ENSEMBLE MODELS - BAGGING ===\n",
    "    'Random_Forest_N50': RandomForestClassifier(n_estimators=50, max_depth=10, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Random_Forest_N100': RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Random_Forest_N200': RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Random_Forest_Deep': RandomForestClassifier(n_estimators=100, max_depth=20, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'Extra_Trees_N100': ExtraTreesClassifier(n_estimators=100, max_depth=10, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    \n",
    "    # === ENSEMBLE MODELS - BOOSTING ===\n",
    "    'AdaBoost_N50': AdaBoostClassifier(n_estimators=50, random_state=42, algorithm='SAMME'),\n",
    "    'AdaBoost_N100': AdaBoostClassifier(n_estimators=100, random_state=42, algorithm='SAMME'),\n",
    "    'GradientBoosting_N50': GradientBoostingClassifier(n_estimators=50, max_depth=3, random_state=42, n_iter_no_change=5, validation_fraction=0.1),\n",
    "    'GradientBoosting_N100': GradientBoostingClassifier(n_estimators=100, max_depth=3, random_state=42, n_iter_no_change=5, validation_fraction=0.1),\n",
    "    'XGBoost_D3_N50': XGBClassifier(max_depth=3, n_estimators=50, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    'XGBoost_D3_N100': XGBClassifier(max_depth=3, n_estimators=100, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    'XGBoost_D5_N100': XGBClassifier(max_depth=5, n_estimators=100, scale_pos_weight=scale_pos_weight, random_state=42, eval_metric='logloss', use_label_encoder=False, tree_method='hist'),\n",
    "    \n",
    "    # === SVM VARIATIONS ===\n",
    "    'SVM_Linear': SVC(kernel='linear', probability=True, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'SVM_RBF_C1': SVC(kernel='rbf', C=1.0, probability=True, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'SVM_RBF_C10': SVC(kernel='rbf', C=10.0, probability=True, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'SVM_Poly_D2': SVC(kernel='poly', degree=2, probability=True, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    'SVM_Poly_D3': SVC(kernel='poly', degree=3, probability=True, random_state=42, class_weight='balanced', n_jobs=-1),\n",
    "    \n",
    "    # === K-NEAREST NEIGHBORS ===\n",
    "    'KNN_K3': KNeighborsClassifier(n_neighbors=3, n_jobs=1),\n",
    "    'KNN_K5': KNeighborsClassifier(n_neighbors=5, n_jobs=1),\n",
    "    'KNN_K7': KNeighborsClassifier(n_neighbors=7, n_jobs=1),\n",
    "    'KNN_K10': KNeighborsClassifier(n_neighbors=10, n_jobs=1),\n",
    "    \n",
    "    # === NEURAL NETWORKS ===\n",
    "    'NN_Small': MLPClassifier(hidden_layer_sizes=(25,), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Medium': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Large': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Deep': MLPClassifier(hidden_layer_sizes=(100, 50, 25), max_iter=500, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='lbfgs'),\n",
    "    'NN_Adam': MLPClassifier(hidden_layer_sizes=(50, 25), max_iter=2000, random_state=42, early_stopping=True, validation_fraction=0.1, n_iter_no_change=10, solver='adam'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7d5474-b532-4962-9c16-c8d2c5125b59",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Train weighted models - WITH TIMING\n",
    "results_weighted = []\n",
    "model_times_weighted = {}\n",
    "\n",
    "for name, model in models_weighted.items():\n",
    "    start_time = time.time()\n",
    "    print(f\"Training {name} (weighted)...\", end=\" \")\n",
    "    \n",
    "    metrics = evaluate_model(model, X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    metrics['model'] = name\n",
    "    results_weighted.append(metrics)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    model_times_weighted[name] = elapsed\n",
    "    \n",
    "    print(f\"‚úì ({elapsed:.2f}s) Test Acc: {metrics['test_acc']:.4f}, AUC: {metrics['test_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n‚è±Ô∏è  Weighted models training time summary:\")\n",
    "fastest_w = min(model_times_weighted, key=model_times_weighted.get)\n",
    "slowest_w = max(model_times_weighted, key=model_times_weighted.get)\n",
    "print(f\"Fastest: {fastest_w} ({model_times_weighted[fastest_w]:.2f}s)\")\n",
    "print(f\"Slowest: {slowest_w} ({model_times_weighted[slowest_w]:.2f}s)\")\n",
    "print(f\"Total time: {sum(model_times_weighted.values()):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f78025-1ad3-47af-9349-54a99773abf5",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create comparison DataFrames\n",
    "results_df_weighted = pd.DataFrame(results_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013587d2-070e-476f-a7b5-39eaa2213f9a",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Merge for side-by-side comparison\n",
    "comparison = pd.merge(\n",
    "    results_df[['model', 'test_acc', 'precision', 'recall', 'f1']],\n",
    "    results_df_weighted[['model', 'test_acc', 'precision', 'recall', 'f1']],\n",
    "    on='model',\n",
    "    suffixes=('_original', '_weighted')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b8512-adf6-4ab3-9de4-977f0c01e8fb",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate improvements\n",
    "comparison['acc_change'] = comparison['test_acc_weighted'] - comparison['test_acc_original']\n",
    "comparison['recall_change'] = comparison['recall_weighted'] - comparison['recall_original']\n",
    "comparison['f1_change'] = comparison['f1_weighted'] - comparison['f1_original']\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9582998c-0ac3-41ad-a713-5a8554aff733",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Detailed confusion matrix comparison\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CONFUSION MATRIX COMPARISON (Original vs Weighted)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for name, model_original in models.items():\n",
    "    model_weighted = models_weighted[name]\n",
    "    \n",
    "    # Get predictions\n",
    "    y_pred_original = model_original.predict(X_test_scaled)\n",
    "    y_pred_weighted = model_weighted.predict(X_test_scaled)\n",
    "    \n",
    "    # Confusion matrices\n",
    "    cm_original = confusion_matrix(y_test, y_pred_original)\n",
    "    cm_weighted = confusion_matrix(y_test, y_pred_weighted)\n",
    "    \n",
    "    print(f\"\\n### {name} ###\")\n",
    "    print(\"\\nOriginal (No Class Weights):\")\n",
    "    print(f\"                Predicted Toxic    Predicted NonToxic\")\n",
    "    print(f\"Actual Toxic          {cm_original[0,0]:3d}                 {cm_original[0,1]:3d}\")\n",
    "    print(f\"Actual NonToxic       {cm_original[1,0]:3d}                 {cm_original[1,1]:3d}\")\n",
    "    \n",
    "    print(\"\\nWith Class Weights:\")\n",
    "    print(f\"                Predicted Toxic    Predicted NonToxic\")\n",
    "    print(f\"Actual Toxic          {cm_weighted[0,0]:3d}                 {cm_weighted[0,1]:3d}\")\n",
    "    print(f\"Actual NonToxic       {cm_weighted[1,0]:3d}                 {cm_weighted[1,1]:3d}\")\n",
    "    \n",
    "    # Calculate recall for minority class\n",
    "    recall_toxic_orig = cm_original[0,0] / (cm_original[0,0] + cm_original[0,1]) if (cm_original[0,0] + cm_original[0,1]) > 0 else 0\n",
    "    recall_toxic_weighted = cm_weighted[0,0] / (cm_weighted[0,0] + cm_weighted[0,1]) if (cm_weighted[0,0] + cm_weighted[0,1]) > 0 else 0\n",
    "    \n",
    "    print(f\"\\nRecall for Toxic class: {recall_toxic_orig:.3f} ‚Üí {recall_toxic_weighted:.3f} (Œî={recall_toxic_weighted-recall_toxic_orig:+.3f})\")\n",
    "    \n",
    "    # False negatives\n",
    "    fn_orig = cm_original[0,1]\n",
    "    fn_weighted = cm_weighted[0,1]\n",
    "    print(f\"False Negatives (Toxic ‚Üí NonToxic): {fn_orig} ‚Üí {fn_weighted} (Œî={fn_weighted-fn_orig:+d})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a337206f-8f05-4d26-a169-f0d9d540a82d",
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-04T05:56:02.252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save all comparisons\n",
    "# comparison.to_csv('class_weight_comparison.csv', index=False)\n",
    "# results_df.to_csv('model_results_original.csv', index=False)\n",
    "# results_df_weighted.to_csv('model_results_weighted.csv', index=False)\n",
    "# print(\"\\n‚úì All results saved to CSV files\")\n",
    "# print(\"  - model_results_original.csv\")\n",
    "# print(\"  - model_results_weighted.csv\")\n",
    "# print(\"  - class_weight_comparison.csv\")\n",
    "# print(\"  - feature_importance_comparison.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28b8fd-d752-48dd-b8e4-4fa64481a352",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a590b3-a7bf-48a9-a497-69b9a70bed43",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51947737-d1b0-4686-9342-980c08d8a51a",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8362584,
     "sourceId": 13195792,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
